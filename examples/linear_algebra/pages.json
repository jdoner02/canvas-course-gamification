{
  "pages": [
    {
      "id": "syllabus",
      "title": "Course Syllabus - MATH 231 Linear Algebra Mastery Journey",
      "body": "<h1>Welcome to Linear Algebra: A Mastery-Based Learning Adventure</h1><p>This course uses a <strong>skill tree approach</strong> where you'll progress through interconnected topics, unlocking new areas as you master prerequisites. Each module includes:</p><ul><li><strong>Mastery Checks:</strong> Demonstrate proficiency before advancing</li><li><strong>Badges & XP:</strong> Earn recognition for achievements</li><li><strong>Multiple Pathways:</strong> Choose your learning route based on interests and strengths</li><li><strong>Real-World Applications:</strong> See how linear algebra connects to your future career</li></ul><h2>Course Structure</h2><p>The course is organized into 12 interconnected modules, each building on previous knowledge. You must achieve at least 75% mastery on most modules to unlock the next level.</p><h2>Assessment Philosophy</h2><p>We focus on <em>mastery over speed</em>. You can retake assessments until you demonstrate proficiency. The goal is deep understanding, not just completion.</p><h2>Gamification Elements</h2><ul><li><strong>Experience Points (XP):</strong> Earn points for completing activities and demonstrating mastery</li><li><strong>Badges:</strong> Collect digital badges for specific achievements</li><li><strong>Leaderboards:</strong> Friendly competition to motivate progress</li><li><strong>Unlockable Content:</strong> Advanced topics become available as you progress</li></ul>"
    },
    {
      "id": "skill-tree-overview",
      "title": "Linear Algebra Skill Tree Overview",
      "body": "<h1>Your Learning Journey Map</h1><p>This interactive skill tree shows your path through Linear Algebra. Each node represents a key concept, and arrows show prerequisite relationships.</p><div class='skill-tree-diagram'><img src='/images/skill-tree-overview.png' alt='Linear Algebra Skill Tree' style='max-width: 100%; height: auto;'/></div><h2>Progression Levels</h2><ol><li><strong>Recognition:</strong> 'I know what this is' - Identify and recall concepts</li><li><strong>Application:</strong> 'I can use this' - Apply procedures in guided problems</li><li><strong>Intuition:</strong> 'I understand why' - Explain reasoning and connections</li><li><strong>Synthesis:</strong> 'I can connect and innovate' - Combine concepts creatively</li><li><strong>Mastery:</strong> 'I can teach this' - Expert-level understanding and application</li></ol><h2>Your Current Progress</h2><p><em>Your personalized progress will be displayed here as you advance through the course.</em></p>"
    },
    {
      "id": "prerequisite-review",
      "title": "Prerequisite Review - Mathematical Foundations",
      "body": "<h1>Essential Mathematical Background</h1><p>Before diving into linear algebra, let's review the key mathematical concepts you'll need.</p><h2>Arithmetic & Basic Algebra</h2><ul><li>Real number operations (addition, multiplication, fractions)</li><li>Solving linear equations: ax + b = 0</li><li>Algebraic manipulation and simplification</li><li>Working with negative numbers and exponents</li></ul><h2>Coordinate Geometry</h2><ul><li>Cartesian coordinate system</li><li>Distance formula: \u221a[(x\u2082-x\u2081)\u00b2 + (y\u2082-y\u2081)\u00b2]</li><li>Slope and intercepts of lines</li><li>Basic graphing skills</li></ul><h2>Functions & Basic Vectors</h2><ul><li>Function notation and evaluation</li><li>Linear and quadratic functions</li><li>Introduction to vectors as ordered pairs (x,y)</li><li>Basic vector addition and scalar multiplication</li></ul><h2>Self-Assessment</h2><p>Complete the prerequisite assessment quiz to identify any areas that need review. Don't worry - we'll provide resources to strengthen any weak areas!</p>"
    },
    {
      "id": "gamification-guide",
      "title": "How to Navigate Your Gamified Learning Experience",
      "body": "<h1>Your Gaming Guide to Linear Algebra</h1><p>This course transforms traditional math learning into an engaging adventure. Here's how to make the most of the gamification features:</p><h2>\ud83c\udfae Core Game Mechanics</h2><h3>Experience Points (XP)</h3><ul><li>Earn XP for completing activities, mastering concepts, and helping classmates</li><li>Track your total XP on the course dashboard</li><li>XP accumulates to unlock special content and privileges</li></ul><h3>\ud83c\udfc6 Badges & Achievements</h3><ul><li><strong>Skill Badges:</strong> Master specific concepts (e.g., 'Vector Warrior', 'Matrix Manipulator')</li><li><strong>Behavior Badges:</strong> Demonstrate good learning habits (e.g., 'Early Bird', 'Help Hero')</li><li><strong>Milestone Badges:</strong> Complete major course sections (e.g., 'Foundation Explorer', 'Transformation Mage')</li></ul><h3>\ud83d\uddfa\ufe0f Mastery Paths</h3><p>Based on your performance, Canvas will automatically guide you to personalized content:</p><ul><li><strong>Express Path:</strong> For those demonstrating strong understanding</li><li><strong>Standard Path:</strong> Balanced progression with practice</li><li><strong>Support Path:</strong> Additional resources and guided practice</li></ul><h2>\ud83d\udcca Progress Tracking</h2><ul><li>Module completion percentages</li><li>Concept mastery indicators</li><li>XP leaderboard (opt-in participation)</li><li>Personal achievement gallery</li></ul><h2>\ud83c\udfaf Tips for Success</h2><ol><li><strong>Aim for Mastery:</strong> 75%+ scores unlock the next level</li><li><strong>Use Retakes:</strong> Keep trying until you achieve mastery</li><li><strong>Explore Bonus Content:</strong> Additional XP and deeper understanding</li><li><strong>Collaborate:</strong> Help others to earn collaboration badges</li></ol>"
    },
    {
      "id": "vectors-definition",
      "title": "What is a Vector? - Building Block of Linear Algebra",
      "body": "<h1>Vectors: Your First Linear Algebra Concept</h1><h2>Definition and Notation</h2><p>A <strong>vector</strong> is an ordered list of numbers that represents both magnitude (size) and direction. In \u211d\u207f, a vector is an n-tuple of real numbers.</p><div class='math-example'><p>Examples:</p><ul><li>In \u211d\u00b2: <strong>v</strong> = (3, 4) or <strong>v</strong> = [3, 4]\u1d40</li><li>In \u211d\u00b3: <strong>u</strong> = (1, -2, 5)</li><li>In \u211d\u207f: <strong>w</strong> = (w\u2081, w\u2082, ..., w\u2099)</li></ul></div><h2>Geometric Interpretation</h2><p>In 2D and 3D, vectors can be visualized as arrows:</p><ul><li><strong>Magnitude:</strong> Length of the arrow</li><li><strong>Direction:</strong> Where the arrow points</li><li><strong>Position Independence:</strong> Vectors can be moved without changing their identity</li></ul><h2>Why Vectors Matter</h2><p>Vectors appear everywhere in science and technology:</p><ul><li><strong>Physics:</strong> Force, velocity, acceleration</li><li><strong>Computer Graphics:</strong> Position, rotation, scaling</li><li><strong>Data Science:</strong> Feature vectors, data points</li><li><strong>Engineering:</strong> Loads, displacements, fields</li></ul><h2>\ud83c\udfaf Learning Checkpoint</h2><p>Can you identify which of these represent vectors and explain why?</p><ol><li>Temperature in a room</li><li>Wind velocity</li><li>Your position on a map</li><li>The number 42</li></ol>"
    },
    {
      "id": "vector-operations",
      "title": "Vector Operations - Addition, Subtraction, and Scalar Multiplication",
      "body": "<h1>Essential Vector Operations</h1><h2>Vector Addition</h2><p>Add vectors component-wise: <strong>u</strong> + <strong>v</strong> = (u\u2081 + v\u2081, u\u2082 + v\u2082, ..., u\u2099 + v\u2099)</p><div class='interactive-example'><p><strong>Example:</strong> (2, 3) + (1, -4) = (3, -1)</p><p><strong>Geometric Interpretation:</strong> Place vectors tip-to-tail or use the parallelogram rule</p></div><h2>Scalar Multiplication</h2><p>Multiply each component by a scalar: c<strong>v</strong> = (cv\u2081, cv\u2082, ..., cv\u2099)</p><ul><li><strong>c > 1:</strong> Stretches the vector</li><li><strong>0 < c < 1:</strong> Shrinks the vector</li><li><strong>c < 0:</strong> Reverses direction and scales</li><li><strong>c = 0:</strong> Results in the zero vector</li></ul><h2>Properties of Vector Operations</h2><div class='property-box'><h3>Addition Properties:</h3><ul><li><strong>Commutative:</strong> <strong>u</strong> + <strong>v</strong> = <strong>v</strong> + <strong>u</strong></li><li><strong>Associative:</strong> (<strong>u</strong> + <strong>v</strong>) + <strong>w</strong> = <strong>u</strong> + (<strong>v</strong> + <strong>w</strong>)</li><li><strong>Zero Vector:</strong> <strong>v</strong> + <strong>0</strong> = <strong>v</strong></li><li><strong>Additive Inverse:</strong> <strong>v</strong> + (-<strong>v</strong>) = <strong>0</strong></li></ul></div><h2>\ud83c\udfae Practice Challenge</h2><p>Try these vector operations and check your geometric intuition:</p><ol><li>Calculate: 3(2, -1) + 2(1, 3)</li><li>Find a vector that when added to (4, 1) gives (0, 5)</li><li>What happens geometrically when you multiply a vector by -1?</li></ol>"
    },
    {
      "id": "dot-product-geometry",
      "title": "The Dot Product - Measuring Angles and Projections",
      "body": "<h1>The Dot Product: Gateway to Geometry</h1><h2>Definition</h2><p>The dot product of two vectors <strong>u</strong> and <strong>v</strong> in \u211d\u207f is:</p><div class='formula-box'><strong>u</strong> \u00b7 <strong>v</strong> = u\u2081v\u2081 + u\u2082v\u2082 + ... + u\u2099v\u2099</div><h2>Geometric Formula</h2><p>The dot product relates to the angle \u03b8 between vectors:</p><div class='formula-box'><strong>u</strong> \u00b7 <strong>v</strong> = ||<strong>u</strong>|| ||<strong>v</strong>|| cos \u03b8</div><p>Where ||<strong>v</strong>|| = \u221a(v\u2081\u00b2 + v\u2082\u00b2 + ... + v\u2099\u00b2) is the magnitude (length) of vector <strong>v</strong>.</p><h2>Key Properties</h2><ul><li><strong>Orthogonality:</strong> <strong>u</strong> \u00b7 <strong>v</strong> = 0 \u27fa <strong>u</strong> \u22a5 <strong>v</strong> (vectors are perpendicular)</li><li><strong>Magnitude:</strong> <strong>v</strong> \u00b7 <strong>v</strong> = ||<strong>v</strong>||\u00b2</li><li><strong>Angle Detection:</strong> cos \u03b8 = (<strong>u</strong> \u00b7 <strong>v</strong>) / (||<strong>u</strong>|| ||<strong>v</strong>||)</li></ul><h2>Applications</h2><div class='application-grid'><div class='app-card'><h3>Physics</h3><p>Work = Force \u00b7 Displacement</p></div><div class='app-card'><h3>Computer Graphics</h3><p>Lighting calculations, surface normals</p></div><div class='app-card'><h3>Data Science</h3><p>Similarity measures, correlation</p></div></div><h2>\ud83c\udfaf Mastery Check</h2><p>Given vectors <strong>a</strong> = (3, 4) and <strong>b</strong> = (1, 2):</p><ol><li>Calculate <strong>a</strong> \u00b7 <strong>b</strong></li><li>Find the angle between them</li><li>Determine if they are orthogonal</li></ol>"
    },
    {
      "id": "linear-combinations",
      "title": "Linear Combinations - Building New Vectors",
      "body": "<h1>Linear Combinations: The Foundation of Vector Spaces</h1><h2>Definition</h2><p>A <strong>linear combination</strong> of vectors v\u2081, v\u2082, ..., v\u2096 is any vector of the form:</p><div class='formula-box'>c\u2081<strong>v\u2081</strong> + c\u2082<strong>v\u2082</strong> + ... + c\u2096<strong>v\u2096</strong></div><p>where c\u2081, c\u2082, ..., c\u2096 are scalars (real numbers).</p><h2>Examples</h2><div class='example-box'><p><strong>Given:</strong> <strong>v\u2081</strong> = (1, 0) and <strong>v\u2082</strong> = (0, 1)</p><p><strong>Linear combinations include:</strong></p><ul><li>2<strong>v\u2081</strong> + 3<strong>v\u2082</strong> = 2(1, 0) + 3(0, 1) = (2, 3)</li><li>-<strong>v\u2081</strong> + 0.5<strong>v\u2082</strong> = (-1, 0.5)</li><li>\u03c0<strong>v\u2081</strong> + \u221a2<strong>v\u2082</strong> = (\u03c0, \u221a2)</li></ul></div><h2>Geometric Interpretation</h2><p>In \u211d\u00b2:</p><ul><li><strong>One vector:</strong> Forms a line through the origin</li><li><strong>Two non-parallel vectors:</strong> Fill the entire plane</li><li><strong>Two parallel vectors:</strong> Still just a line</li></ul><h2>Key Questions</h2><ol><li><strong>Can we express a target vector as a linear combination?</strong></li><li><strong>What's the smallest set of vectors needed?</strong></li><li><strong>When do we get 'redundant' vectors?</strong></li></ol><h2>\ud83c\udfae Interactive Challenge</h2><p>Express the vector (5, 2) as a linear combination of:</p><ul><li><strong>v\u2081</strong> = (1, 1) and <strong>v\u2082</strong> = (1, -1)</li></ul><p><em>Hint: Set up the equation c\u2081(1, 1) + c\u2082(1, -1) = (5, 2) and solve!</em></p>"
    },
    {
      "id": "span-concept",
      "title": "Span - The Set of All Possible Linear Combinations",
      "body": "<h1>Span: Exploring All Possibilities</h1><h2>Definition</h2><p>The <strong>span</strong> of a set of vectors {v\u2081, v\u2082, ..., v\u2096} is the set of ALL possible linear combinations of these vectors.</p><div class='formula-box'>Span{v\u2081, v\u2082, ..., v\u2096} = {c\u2081v\u2081 + c\u2082v\u2082 + ... + c\u2096v\u2096 : c\u2081, c\u2082, ..., c\u2096 \u2208 \u211d}</div><h2>Geometric Visualization</h2><table class=\"span-table\" role=\"table\" aria-label=\"Vector span visualization table\"><caption>Table showing the relationship between number of vectors and their span in \u211d\u00b3</caption><thead><tr><th scope=\"col\">Number of Vectors</th><th scope=\"col\">Typical Span in \u211d\u00b3</th><th scope=\"col\">Dimension</th></tr></thead><tbody><tr><td>1 non-zero vector</td><td>Line through origin</td><td>1</td></tr><tr><td>2 linearly independent</td><td>Plane through origin</td><td>2</td></tr><tr><td>3 linearly independent</td><td>All of \u211d\u00b3</td><td>3</td></tr></tbody></table><h2>Important Properties</h2><ul><li><strong>Always includes the zero vector:</strong> Set all coefficients to 0</li><li><strong>Closed under addition:</strong> Sum of vectors in span is also in span</li><li><strong>Closed under scalar multiplication:</strong> Scalar multiple of vector in span is also in span</li></ul><h2>Questions to Ask</h2><ol><li><strong>Does vector <strong>w</strong> lie in Span{v\u2081, v\u2082}?</strong> \u27f9 Solve c\u2081v\u2081 + c\u2082v\u2082 = w</li><li><strong>Do these vectors span all of \u211d\u207f?</strong> \u27f9 Check if we can express any vector</li><li><strong>What's the 'shape' of this span?</strong> \u27f9 Line, plane, or higher dimension?</li></ol><h2>\ud83c\udfaf Challenge Problem</h2><p>Determine if the vector (1, 2, 3) is in the span of:</p><ul><li><strong>v\u2081</strong> = (1, 0, 1)</li><li><strong>v\u2082</strong> = (0, 1, 1)</li><li><strong>v\u2083</strong> = (1, 1, 0)</li></ul><p><strong>Method:</strong> Set up the linear system and solve using techniques from the next module!</p>"
    },
    {
      "id": "linear-systems-intro",
      "title": "Systems of Linear Equations - Setting Up the Problem",
      "body": "<h1>Linear Systems: Where Algebra Meets Geometry</h1><h2>What is a Linear System?</h2><p>A system of linear equations is a collection of linear equations involving the same variables.</p><div class='system-example'><h3>Example System:</h3><p>x + 2y - z = 3</p><p>2x - y + z = 0</p><p>-x + y + 2z = -1</p></div><h2>Matrix Form: Ax = b</h2><p>We can represent this system using matrices:</p><div class='matrix-representation'><p><strong>Coefficient Matrix A:</strong> Contains the coefficients</p><p><strong>Variable Vector x:</strong> Contains the unknowns</p><p><strong>Constant Vector b:</strong> Contains the right-hand sides</p></div><h2>Types of Solutions</h2><ol><li><strong>Unique Solution:</strong> Lines/planes intersect at exactly one point</li><li><strong>No Solution:</strong> Inconsistent system (parallel lines/planes)</li><li><strong>Infinite Solutions:</strong> Lines/planes coincide or intersect along a line/plane</li></ol><h2>Geometric Interpretation</h2><div class='geometry-grid'><div class='geo-card'><h3>2 Variables</h3><p>Each equation represents a line. Solution is intersection point(s).</p></div><div class='geo-card'><h3>3 Variables</h3><p>Each equation represents a plane. Solution is intersection of all planes.</p></div><div class='geo-card'><h3>n Variables</h3><p>Each equation represents a hyperplane in \u211d\u207f.</p></div></div><h2>\ud83c\udfae Real-World Connection</h2><p><strong>Traffic Flow Problem:</strong> Model traffic through intersections using linear systems where variables represent flow rates and equations represent conservation of vehicles.</p><h2>\ud83c\udfaf Check Your Understanding</h2><p>Write the following system in matrix form Ax = b:</p><p>3x - y = 7</p><p>x + 2y = 1</p>"
    },
    {
      "id": "gaussian-elimination",
      "title": "Gaussian Elimination - The Systematic Solution Method",
      "body": "<h1>Gaussian Elimination: The Universal Solver</h1><h2>The Algorithm</h2><p>Gaussian Elimination uses three <strong>elementary row operations</strong> to systematically solve linear systems:</p><ol><li><strong>Row Swap:</strong> R\u1d62 \u2194 R\u2c7c</li><li><strong>Row Scaling:</strong> cR\u1d62 \u2192 R\u1d62 (c \u2260 0)</li><li><strong>Row Addition:</strong> R\u1d62 + cR\u2c7c \u2192 R\u1d62</li></ol><h2>Step-by-Step Process</h2><div class='algorithm-steps'><h3>Goal: Transform to Row Echelon Form (REF)</h3><ol><li><strong>Create leading 1:</strong> Make first nonzero entry in each row equal to 1</li><li><strong>Create zeros below:</strong> Eliminate entries below each leading 1</li><li><strong>Move right:</strong> Work column by column from left to right</li></ol></div><h2>Example Walkthrough</h2><div class='elimination-example'><h3>Solve:</h3><table role=\"table\" aria-label=\"Linear system to matrix transformation example\"><caption>Example showing conversion from system of equations to augmented matrix form</caption><thead><tr><th scope=\"col\">System of Equations</th><th scope=\"col\">Arrow</th><th scope=\"col\">Matrix Form</th></tr></thead><tbody><tr><td>x + 2y - z = 3</td><td rowspan='3'>\u2192</td><td>Augmented Matrix:</td></tr><tr><td>2x - y + z = 0</td><td>[1  2 -1 |  3]</td></tr><tr><td>-x + y + 2z = -1</td><td>[2 -1  1 |  0]</td></tr><tr><td></td><td></td><td>[-1  1  2 | -1]</td></tr></tbody></table></div><h2>Row Echelon Form (REF)</h2><ul><li>All nonzero rows above rows of all zeros</li><li>Leading entry of each row is to the right of leading entry above it</li><li>All entries below a leading entry are zero</li></ul><h2>Reduced Row Echelon Form (RREF)</h2><ul><li>Satisfies REF conditions</li><li>Leading entry in each row is 1</li><li>All other entries in columns with leading 1s are zero</li></ul><h2>\ud83c\udfae Practice Game</h2><p>Transform this augmented matrix to RREF:</p><p>[2  4  2 |  8]</p><p>[1  1  3 |  5]</p><p>[3  5  1 | 11]</p><p><em>Track your moves and try to minimize the number of operations!</em></p>"
    },
    {
      "id": "applications-overview",
      "title": "Linear Algebra in the Real World",
      "body": "<h1>Where Linear Algebra Powers Our World</h1><h2>\ud83c\udfae Computer Graphics & Gaming</h2><ul><li><strong>3D Transformations:</strong> Rotation, scaling, translation matrices</li><li><strong>Computer Vision:</strong> Image processing and object recognition</li><li><strong>Animation:</strong> Skeletal animation using transformation matrices</li><li><strong>Virtual Reality:</strong> Real-time 3D rendering</li></ul><h2>\ud83d\udcca Data Science & Machine Learning</h2><ul><li><strong>Principal Component Analysis (PCA):</strong> Dimensionality reduction using eigenvalues</li><li><strong>Linear Regression:</strong> Finding best-fit lines using normal equations</li><li><strong>Neural Networks:</strong> Matrix operations in forward/backward propagation</li><li><strong>Recommendation Systems:</strong> Matrix factorization techniques</li></ul><h2>\ud83c\udfd7\ufe0f Engineering Applications</h2><ul><li><strong>Structural Analysis:</strong> Solving systems for forces and displacements</li><li><strong>Circuit Analysis:</strong> Kirchhoff's laws as linear systems</li><li><strong>Control Systems:</strong> State-space representations</li><li><strong>Signal Processing:</strong> Fourier transforms and filtering</li></ul><h2>\ud83d\udcb0 Business & Economics</h2><ul><li><strong>Supply Chain Optimization:</strong> Linear programming</li><li><strong>Portfolio Optimization:</strong> Risk-return analysis using covariance matrices</li><li><strong>Market Analysis:</strong> Factor models and correlation analysis</li></ul><h2>\ud83d\udd12 Cybersecurity & Cryptography</h2><ul><li><strong>Error-Correcting Codes:</strong> Vector spaces over finite fields</li><li><strong>Hill Cipher:</strong> Matrix-based encryption</li><li><strong>Network Security:</strong> Graph theory and adjacency matrices</li></ul><h2>\ud83c\udfaf Career Connections</h2><p>Select your career interest to see specific linear algebra applications:</p><div class='career-selector'><button>Software Engineer</button><button>Data Scientist</button><button>Mechanical Engineer</button><button>Financial Analyst</button><button>Game Developer</button></div>"
    },
    {
      "id": "vectors-applications",
      "title": "Vector Applications in Science and Technology",
      "body": "<h1>Vectors in Action</h1><h2>\ud83d\ude80 Physics Applications</h2><ul><li><strong>Force Vectors:</strong> Combining multiple forces acting on an object</li><li><strong>Velocity and Acceleration:</strong> Motion in multiple dimensions</li><li><strong>Electric and Magnetic Fields:</strong> Vector field representations</li></ul><h2>\ud83c\udfae Computer Graphics</h2><ul><li><strong>Position Vectors:</strong> Locating objects in 3D space</li><li><strong>Normal Vectors:</strong> Surface lighting calculations</li><li><strong>Direction Vectors:</strong> Camera orientation and movement</li></ul><h2>\ud83d\udcca Data Science</h2><ul><li><strong>Feature Vectors:</strong> Representing data points in machine learning</li><li><strong>Word Embeddings:</strong> Representing words as high-dimensional vectors</li><li><strong>Principal Components:</strong> Dimensionality reduction directions</li></ul><h2>\ud83c\udfaf Interactive Examples</h2><p>Explore how vectors are used in real-world scenarios through interactive simulations and practice problems.</p>"
    },
    {
      "id": "subspace-introduction",
      "title": "Introduction to Subspaces",
      "body": "<h1>Subspaces: Vector Spaces Within Vector Spaces</h1><h2>What is a Subspace?</h2><p>A subspace of a vector space V is a subset H of V that is itself a vector space under the same operations.</p><h2>The Subspace Test</h2><p>A subset H of vector space V is a subspace if and only if:</p><ol><li><strong>Zero vector:</strong> <strong>0</strong> \u2208 H</li><li><strong>Closure under addition:</strong> If <strong>u</strong>, <strong>v</strong> \u2208 H, then <strong>u</strong> + <strong>v</strong> \u2208 H</li><li><strong>Closure under scalar multiplication:</strong> If <strong>v</strong> \u2208 H and c is a scalar, then c<strong>v</strong> \u2208 H</li></ol><h2>Common Examples</h2><ul><li><strong>The span of any set of vectors</strong> is always a subspace</li><li><strong>Lines through the origin</strong> in \u211d\u00b2</li><li><strong>Planes through the origin</strong> in \u211d\u00b3</li><li><strong>The null space</strong> of a matrix</li></ul><h2>\ud83c\udfaf Practice</h2><p>Determine if these sets are subspaces of \u211d\u00b2:</p><ol><li>All vectors of the form (a, 2a)</li><li>All vectors (x, y) where x \u2265 0</li><li>All vectors (x, y) where x + y = 1</li></ol>"
    },
    {
      "id": "augmented-matrices",
      "title": "Augmented Matrices and Row Operations",
      "body": "<h1>Augmented Matrices: Organizing Linear Systems</h1><h2>What is an Augmented Matrix?</h2><p>An augmented matrix combines the coefficient matrix and the constant vector into one rectangular array:</p><div class='matrix-example'><p>System: ax + by = e, cx + dy = f</p><p>Augmented Matrix: [a b | e]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[c d | f]</p></div><h2>Elementary Row Operations</h2><ol><li><strong>Row Swap:</strong> R\u2081 \u2194 R\u2082</li><li><strong>Row Multiplication:</strong> kR\u2081 \u2192 R\u2081 (k \u2260 0)</li><li><strong>Row Addition:</strong> R\u2081 + kR\u2082 \u2192 R\u2081</li></ol><h2>Why These Operations Work</h2><p>Each row operation corresponds to a valid manipulation of the original system of equations that preserves the solution set.</p><h2>\ud83c\udfae Interactive Practice</h2><p>Practice performing row operations on augmented matrices to solve systems step by step.</p>"
    },
    {
      "id": "solution-interpretation",
      "title": "Interpreting Solutions of Linear Systems",
      "body": "<h1>Understanding What Solutions Tell Us</h1><h2>Types of Solution Sets</h2><ol><li><strong>Unique Solution:</strong> Exactly one solution vector</li><li><strong>No Solution:</strong> Inconsistent system (0 = 1 appears in RREF)</li><li><strong>Infinite Solutions:</strong> Free variables create a parametric solution</li></ol><h2>Parametric Solutions</h2><p>When there are free variables, express the solution in parametric form:</p><div class='parametric-example'><p>If x\u2083 is free, then:</p><p>x\u2081 = 2 - 3x\u2083</p><p>x\u2082 = 1 + x\u2083</p><p>x\u2083 = t (parameter)</p></div><h2>Geometric Interpretation</h2><ul><li><strong>2 equations, 2 unknowns:</strong> Lines intersecting at a point, parallel, or coincident</li><li><strong>3 equations, 3 unknowns:</strong> Planes intersecting at a point, along a line, or nowhere</li></ul><h2>\ud83c\udfaf Solution Analysis</h2><p>Given RREF form, determine the type of solution and express it appropriately.</p>"
    },
    {
      "id": "matrix-definition",
      "title": "What is a Matrix? Definition and Notation",
      "body": "<h1>Matrices: Rectangular Arrays of Numbers</h1><h2>Definition</h2><p>A matrix is a rectangular array of numbers arranged in rows and columns. An m\u00d7n matrix has m rows and n columns.</p><h2>Matrix Notation</h2><div class='matrix-notation'><p>General m\u00d7n matrix A:</p><p>A = [a\u2081\u2081 a\u2081\u2082 \u22ef a\u2081\u2099]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[a\u2082\u2081 a\u2082\u2082 \u22ef a\u2082\u2099]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[\u22ee&nbsp;&nbsp;&nbsp;&nbsp;\u22ee&nbsp;&nbsp;&nbsp;&nbsp;\u22f1&nbsp;&nbsp;&nbsp;\u22ee&nbsp;&nbsp;]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[a\u2098\u2081 a\u2098\u2082 \u22ef a\u2098\u2099]</p></div><h2>Special Types of Matrices</h2><ul><li><strong>Square Matrix:</strong> m = n (same number of rows and columns)</li><li><strong>Column Vector:</strong> n = 1 (single column)</li><li><strong>Row Vector:</strong> m = 1 (single row)</li><li><strong>Zero Matrix:</strong> All entries are zero</li></ul><h2>Matrix Equality</h2><p>Two matrices are equal if they have the same dimensions and corresponding entries are equal.</p><h2>\ud83c\udfae Recognition Game</h2><p>Identify the dimensions and special properties of various matrices presented in random order.</p>"
    },
    {
      "id": "matrix-operations",
      "title": "Matrix Operations: Addition, Subtraction, and Scalar Multiplication",
      "body": "<h1>Basic Matrix Operations</h1><h2>Matrix Addition and Subtraction</h2><p>Matrices can be added or subtracted if they have the same dimensions. Operations are performed element-wise:</p><div class='matrix-operation'><p>(A + B)\u1d62\u2c7c = A\u1d62\u2c7c + B\u1d62\u2c7c</p><p>(A - B)\u1d62\u2c7c = A\u1d62\u2c7c - B\u1d62\u2c7c</p></div><h2>Scalar Multiplication</h2><p>Multiply each entry of the matrix by the scalar:</p><div class='scalar-mult'><p>(cA)\u1d62\u2c7c = c\u00b7A\u1d62\u2c7c</p></div><h2>Properties</h2><ul><li><strong>Commutative:</strong> A + B = B + A</li><li><strong>Associative:</strong> (A + B) + C = A + (B + C)</li><li><strong>Distributive:</strong> c(A + B) = cA + cB</li><li><strong>Zero Matrix:</strong> A + O = A</li></ul><h2>\ud83c\udfaf Practice Problems</h2><p>Perform various matrix operations and verify the properties with specific examples.</p>"
    },
    {
      "id": "matrix-properties",
      "title": "Matrix Properties and Special Matrices",
      "body": "<h1>Important Matrix Properties and Types</h1><h2>Special Matrices</h2><ul><li><strong>Identity Matrix I:</strong> Square matrix with 1s on diagonal, 0s elsewhere</li><li><strong>Diagonal Matrix:</strong> Non-zero entries only on the main diagonal</li><li><strong>Triangular Matrices:</strong> Upper or lower triangular</li><li><strong>Symmetric Matrix:</strong> A = A\u1d40 (equal to its transpose)</li></ul><h2>Matrix Transpose</h2><p>The transpose A\u1d40 of matrix A is obtained by swapping rows and columns:</p><p>(A\u1d40)\u1d62\u2c7c = A\u2c7c\u1d62</p><h2>Transpose Properties</h2><ul><li>(A\u1d40)\u1d40 = A</li><li>(A + B)\u1d40 = A\u1d40 + B\u1d40</li><li>(cA)\u1d40 = cA\u1d40</li><li>(AB)\u1d40 = B\u1d40A\u1d40</li></ul><h2>\ud83c\udfae Matrix Type Detective</h2><p>Given various matrices, identify their special properties and classify them.</p>"
    },
    {
      "id": "linear-independence",
      "title": "Linear Independence: When Vectors Don't Depend on Each Other",
      "body": "<h1>Linear Independence: The Key to Efficiency</h1><h2>Definition</h2><p>Vectors v\u2081, v\u2082, ..., v\u2096 are <strong>linearly independent</strong> if the only solution to:</p><p>c\u2081v\u2081 + c\u2082v\u2082 + ... + c\u2096v\u2096 = <strong>0</strong></p><p>is c\u2081 = c\u2082 = ... = c\u2096 = 0.</p><h2>Linear Dependence</h2><p>Vectors are <strong>linearly dependent</strong> if there exist scalars (not all zero) such that the linear combination equals zero. This means one vector can be written as a linear combination of the others.</p><h2>Geometric Interpretation</h2><ul><li><strong>2 vectors in \u211d\u00b2:</strong> Independent if not parallel</li><li><strong>3 vectors in \u211d\u00b3:</strong> Independent if not coplanar</li><li><strong>More than n vectors in \u211d\u207f:</strong> Always dependent</li></ul><h2>Testing for Independence</h2><ol><li>Set up the equation c\u2081v\u2081 + c\u2082v\u2082 + ... + c\u2096v\u2096 = 0</li><li>Write as a homogeneous system</li><li>Find RREF of coefficient matrix</li><li>Independent \u27fa only trivial solution</li></ol><h2>\ud83c\udfaf Independence Detective</h2><p>Determine whether given sets of vectors are linearly independent or dependent.</p>"
    },
    {
      "id": "basis-definition",
      "title": "Basis: The Minimal Spanning Set",
      "body": "<h1>Basis: Efficient Building Blocks for Vector Spaces</h1><h2>Definition</h2><p>A <strong>basis</strong> for a vector space V is a set of vectors that:</p><ol><li><strong>Spans V:</strong> Every vector in V can be expressed as a linear combination</li><li><strong>Is linearly independent:</strong> No vector is redundant</li></ol><h2>Key Properties</h2><ul><li><strong>Uniqueness of Representation:</strong> Every vector has a unique representation in terms of basis vectors</li><li><strong>Same Size:</strong> All bases for the same vector space have the same number of vectors</li><li><strong>Efficiency:</strong> A basis is the smallest set that spans the space</li></ul><h2>Standard Bases</h2><div class='standard-bases'><p><strong>\u211d\u00b2:</strong> {(1,0), (0,1)} = {e\u2081, e\u2082}</p><p><strong>\u211d\u00b3:</strong> {(1,0,0), (0,1,0), (0,0,1)} = {e\u2081, e\u2082, e\u2083}</p><p><strong>\u211d\u207f:</strong> {e\u2081, e\u2082, ..., e\u2099} where e\u1d62 has 1 in position i, 0s elsewhere</p></div><h2>Finding a Basis</h2><ol><li>Start with a spanning set</li><li>Remove dependent vectors one by one</li><li>What remains is a basis</li></ol><h2>\ud83c\udfae Basis Builder Challenge</h2><p>Given a set of vectors, determine if they form a basis, and if not, modify the set to create one.</p>"
    },
    {
      "id": "dimension-coordinates",
      "title": "Dimension and Coordinate Systems",
      "body": "<h1>Dimension: Measuring the Size of Vector Spaces</h1><h2>Definition of Dimension</h2><p>The <strong>dimension</strong> of a vector space V is the number of vectors in any basis for V.</p><h2>Coordinate Representation</h2><p>Given a basis B = {b\u2081, b\u2082, ..., b\u2099} for vector space V, every vector v \u2208 V can be uniquely written as:</p><p>v = c\u2081b\u2081 + c\u2082b\u2082 + ... + c\u2099b\u2099</p><p>The scalars (c\u2081, c\u2082, ..., c\u2099) are the <strong>coordinates of v relative to basis B</strong>.</p><h2>Change of Basis</h2><p>Different bases give different coordinate representations for the same vector. The change of basis matrix converts coordinates between different bases.</p><h2>Dimension Facts</h2><ul><li><strong>\u211d\u207f has dimension n</strong></li><li><strong>A line through origin has dimension 1</strong></li><li><strong>A plane through origin has dimension 2</strong></li><li><strong>The zero space {0} has dimension 0</strong></li></ul><h2>\ud83c\udfaf Coordinate Conversion</h2><p>Practice converting vectors between different coordinate systems and bases.</p>"
    },
    {
      "id": "matrix-inverse",
      "title": "Matrix Inverses: Undoing Matrix Operations",
      "body": "<h1>Matrix Inverses: The Multiplicative 'Undo' Operation</h1><h2>Definition</h2><p>For a square matrix A, the inverse A\u207b\u00b9 (if it exists) satisfies:</p><p>AA\u207b\u00b9 = A\u207b\u00b9A = I</p><p>where I is the identity matrix.</p><h2>When Does an Inverse Exist?</h2><p>A square matrix A is <strong>invertible</strong> (or <strong>nonsingular</strong>) if and only if:</p><ul><li>det(A) \u2260 0</li><li>A has full rank</li><li>The columns of A are linearly independent</li><li>Ax = 0 has only the trivial solution</li></ul><h2>Finding Inverses</h2><h3>2\u00d72 Case:</h3><p>For A = [a b; c d], if ad - bc \u2260 0:</p><p>A\u207b\u00b9 = (1/(ad-bc)) [d -b; -c a]</p><h3>General Case: Gauss-Jordan Method</h3><ol><li>Form augmented matrix [A | I]</li><li>Row reduce to [I | A\u207b\u00b9]</li><li>If impossible to reach [I | ?], then A is not invertible</li></ol><h2>Properties of Inverses</h2><ul><li>(A\u207b\u00b9)\u207b\u00b9 = A</li><li>(AB)\u207b\u00b9 = B\u207b\u00b9A\u207b\u00b9</li><li>(A\u1d40)\u207b\u00b9 = (A\u207b\u00b9)\u1d40</li></ul><h2>\ud83c\udfae Inverse Calculator Challenge</h2><p>Find inverses of various matrices using different methods and verify your results.</p>"
    },
    {
      "id": "determinants",
      "title": "Determinants: Measuring Matrix 'Size' and Orientation",
      "body": "<h1>Determinants: The Scalar Signature of a Matrix</h1><h2>Definition and Notation</h2><p>The determinant of a square matrix A, denoted det(A) or |A|, is a scalar that encodes important information about the matrix.</p><h2>Computing Determinants</h2><h3>2\u00d72 Case:</h3><p>det([a b; c d]) = ad - bc</p><h3>3\u00d73 Case (Cofactor Expansion):</h3><p>det(A) = a\u2081\u2081C\u2081\u2081 + a\u2081\u2082C\u2081\u2082 + a\u2081\u2083C\u2081\u2083</p><p>where C\u1d62\u2c7c = (-1)\u2071\u207a\u02b2 M\u1d62\u2c7c and M\u1d62\u2c7c is the (i,j)-minor</p><h3>General: Laplace Expansion</h3><p>Expand along any row or column using cofactors.</p><h2>Properties of Determinants</h2><ul><li><strong>Row Operations:</strong> Swapping rows changes sign, scaling a row scales det</li><li><strong>Triangular Matrices:</strong> det = product of diagonal entries</li><li><strong>Product Rule:</strong> det(AB) = det(A)det(B)</li><li><strong>Transpose:</strong> det(A\u1d40) = det(A)</li></ul><h2>Geometric Interpretation</h2><ul><li><strong>2\u00d72:</strong> Signed area of parallelogram formed by column vectors</li><li><strong>3\u00d73:</strong> Signed volume of parallelepiped formed by column vectors</li><li><strong>General:</strong> n-dimensional signed volume</li></ul><h2>\ud83c\udfaf Determinant Detective</h2><p>Calculate determinants using various methods and understand their geometric meaning.</p>"
    },
    {
      "id": "inverse-determinant-connection",
      "title": "The Connection Between Inverses and Determinants",
      "body": "<h1>How Inverses and Determinants Work Together</h1><h2>The Fundamental Connection</h2><p>A square matrix A is invertible if and only if det(A) \u2260 0.</p><h2>Cramer's Rule</h2><p>For the system Ax = b where A is invertible:</p><p>x\u1d62 = det(A\u1d62)/det(A)</p><p>where A\u1d62 is A with column i replaced by b.</p><h2>Adjugate Matrix Formula</h2><p>For invertible matrix A:</p><p>A\u207b\u00b9 = (1/det(A)) adj(A)</p><p>where adj(A) is the adjugate (transpose of cofactor matrix).</p><h2>Applications</h2><ul><li><strong>Solving Linear Systems:</strong> When det(A) \u2260 0, unique solution exists</li><li><strong>Change of Variables:</strong> Determinant gives scaling factor</li><li><strong>Eigenvalue Problems:</strong> det(A - \u03bbI) = 0 for eigenvalues</li></ul><h2>\ud83c\udfae Connection Explorer</h2><p>Explore how determinant values affect matrix invertibility and solution existence.</p>"
    },
    {
      "id": "vector-spaces",
      "title": "Vector Spaces: The Abstract Framework",
      "body": "<h1>Vector Spaces: Abstracting the Essential Properties</h1><h2>Definition</h2><p>A vector space V over a field F is a set with two operations (addition and scalar multiplication) satisfying eight axioms:</p><h2>Vector Space Axioms</h2><ol><li><strong>Closure under addition:</strong> u + v \u2208 V</li><li><strong>Commutativity:</strong> u + v = v + u</li><li><strong>Associativity:</strong> (u + v) + w = u + (v + w)</li><li><strong>Zero vector:</strong> \u2203 0 such that v + 0 = v</li><li><strong>Additive inverse:</strong> \u2203 -v such that v + (-v) = 0</li><li><strong>Closure under scalar mult:</strong> cv \u2208 V</li><li><strong>Distributivity:</strong> c(u + v) = cu + cv and (c + d)v = cv + dv</li><li><strong>Scalar associativity:</strong> c(dv) = (cd)v and 1v = v</li></ol><h2>Examples of Vector Spaces</h2><ul><li><strong>\u211d\u207f:</strong> n-tuples of real numbers</li><li><strong>Matrices:</strong> m\u00d7n matrices with matrix addition</li><li><strong>Polynomials:</strong> P\u2099 = polynomials of degree \u2264 n</li><li><strong>Functions:</strong> Real-valued functions on an interval</li></ul><h2>\ud83c\udfaf Vector Space Verification</h2><p>Verify that given sets with operations satisfy the vector space axioms.</p>"
    },
    {
      "id": "subspaces-formal",
      "title": "Subspaces: Formal Definition and Properties",
      "body": "<h1>Subspaces: Vector Spaces Within Vector Spaces</h1><h2>Formal Definition</h2><p>A subset H of vector space V is a subspace if H is itself a vector space under the same operations as V.</p><h2>Subspace Test (Simplified)</h2><p>H \u2286 V is a subspace if and only if:</p><ol><li><strong>0 \u2208 H</strong> (contains zero vector)</li><li><strong>Closed under addition:</strong> u, v \u2208 H \u27f9 u + v \u2208 H</li><li><strong>Closed under scalar multiplication:</strong> v \u2208 H, c \u2208 \u211d \u27f9 cv \u2208 H</li></ol><h2>Important Subspaces</h2><ul><li><strong>Span:</strong> Span{v\u2081, ..., v\u2096} is always a subspace</li><li><strong>Null space:</strong> Null(A) = {x : Ax = 0}</li><li><strong>Column space:</strong> Col(A) = span of columns of A</li><li><strong>Row space:</strong> Row(A) = span of rows of A</li></ul><h2>Operations on Subspaces</h2><ul><li><strong>Intersection:</strong> H\u2081 \u2229 H\u2082 is a subspace</li><li><strong>Sum:</strong> H\u2081 + H\u2082 = {h\u2081 + h\u2082 : h\u2081 \u2208 H\u2081, h\u2082 \u2208 H\u2082}</li></ul><h2>\ud83c\udfae Subspace Detective</h2><p>Determine which subsets are actually subspaces and find their dimensions.</p>"
    },
    {
      "id": "null-column-spaces",
      "title": "Null Space and Column Space of a Matrix",
      "body": "<h1>Key Matrix Subspaces: Null and Column Spaces</h1><h2>Null Space (Kernel)</h2><p><strong>Definition:</strong> Null(A) = {x \u2208 \u211d\u207f : Ax = 0}</p><p>The null space consists of all vectors that A maps to the zero vector.</p><h3>Finding the Null Space:</h3><ol><li>Solve the homogeneous system Ax = 0</li><li>Express solution in parametric form</li><li>The parameter vectors form a basis for Null(A)</li></ol><h2>Column Space</h2><p><strong>Definition:</strong> Col(A) = span{columns of A}</p><p>The column space consists of all possible outputs of the transformation x \u21a6 Ax.</p><h3>Finding the Column Space:</h3><ol><li>Find RREF of A</li><li>Identify pivot columns in RREF</li><li>Corresponding columns in original A form a basis for Col(A)</li></ol><h2>The Rank-Nullity Theorem</h2><p>For m\u00d7n matrix A:</p><p><strong>rank(A) + nullity(A) = n</strong></p><p>where rank(A) = dim(Col(A)) and nullity(A) = dim(Null(A))</p><h2>\ud83c\udfaf Space Explorer</h2><p>Find bases for null and column spaces of given matrices and verify the rank-nullity theorem.</p>"
    },
    {
      "id": "linear-transformations",
      "title": "Linear Transformations: Functions That Preserve Structure",
      "body": "<h1>Linear Transformations: Structure-Preserving Functions</h1><h2>Definition</h2><p>A function T: V \u2192 W between vector spaces is <strong>linear</strong> if:</p><ol><li><strong>Additivity:</strong> T(u + v) = T(u) + T(v)</li><li><strong>Homogeneity:</strong> T(cv) = cT(v)</li></ol><p>Equivalently: T(cu + dv) = cT(u) + dT(v)</p><h2>Matrix Representation</h2><p>Every linear transformation T: \u211d\u207f \u2192 \u211d\u1d50 can be represented as T(x) = Ax for some m\u00d7n matrix A.</p><p>The columns of A are T(e\u2081), T(e\u2082), ..., T(e\u2099) where {e\u2081, ..., e\u2099} is the standard basis.</p><h2>Examples of Linear Transformations</h2><ul><li><strong>Rotation:</strong> Rotating vectors in the plane</li><li><strong>Reflection:</strong> Reflecting across a line or plane</li><li><strong>Scaling:</strong> Stretching or shrinking by a factor</li><li><strong>Projection:</strong> Projecting onto a line or plane</li><li><strong>Shear:</strong> Skewing the coordinate system</li></ul><h2>Properties</h2><ul><li>T(0) = 0 (linear transformations map zero to zero)</li><li>T preserves linear combinations</li><li>T is completely determined by where it sends basis vectors</li></ul><h2>\ud83c\udfae Transformation Visualizer</h2><p>Explore how different matrices transform the unit square and see the geometric effects.</p>"
    },
    {
      "id": "transformation-matrices",
      "title": "Matrix Representations of Linear Transformations",
      "body": "<h1>From Transformations to Matrices</h1><h2>Standard Matrix of a Linear Transformation</h2><p>For T: \u211d\u207f \u2192 \u211d\u1d50, the standard matrix A is constructed by:</p><p>A = [T(e\u2081) T(e\u2082) ... T(e\u2099)]</p><p>where e\u2081, e\u2082, ..., e\u2099 are the standard basis vectors.</p><h2>Common Transformation Matrices</h2><h3>2D Rotations (angle \u03b8):</h3><p>[cos \u03b8  -sin \u03b8]</p><p>[sin \u03b8   cos \u03b8]</p><h3>2D Reflections:</h3><ul><li><strong>Across x-axis:</strong> [1 0; 0 -1]</li><li><strong>Across y-axis:</strong> [-1 0; 0 1]</li><li><strong>Across y = x:</strong> [0 1; 1 0]</li></ul><h3>Scaling:</h3><p>[a 0; 0 b] scales by factor a in x-direction, b in y-direction</p><h2>Composition of Transformations</h2><p>If S: \u211d\u207f \u2192 \u211d\u1d56 and T: \u211d\u1d56 \u2192 \u211d\u1d50, then:</p><p>(T \u2218 S)(x) = T(S(x))</p><p>Matrix representation: [T \u2218 S] = [T][S]</p><h2>\ud83c\udfaf Matrix Builder</h2><p>Given a geometric transformation description, construct its matrix representation.</p>"
    },
    {
      "id": "kernel-range",
      "title": "Kernel and Range of Linear Transformations",
      "body": "<h1>Kernel and Range: Understanding What Transformations Do</h1><h2>Kernel (Null Space)</h2><p><strong>Definition:</strong> ker(T) = {x \u2208 V : T(x) = 0}</p><p>The kernel consists of all vectors that T maps to the zero vector.</p><h3>Properties:</h3><ul><li>ker(T) is always a subspace of the domain</li><li>T is one-to-one \u27fa ker(T) = {0}</li><li>For matrix transformation T(x) = Ax, ker(T) = Null(A)</li></ul><h2>Range (Image)</h2><p><strong>Definition:</strong> range(T) = {T(x) : x \u2208 V} = {y \u2208 W : T(x) = y for some x}</p><p>The range consists of all possible outputs of T.</p><h3>Properties:</h3><ul><li>range(T) is always a subspace of the codomain</li><li>T is onto \u27fa range(T) = W</li><li>For matrix transformation T(x) = Ax, range(T) = Col(A)</li></ul><h2>Rank-Nullity Theorem</h2><p>For linear transformation T: V \u2192 W where dim(V) = n:</p><p><strong>dim(ker(T)) + dim(range(T)) = n</strong></p><h2>\ud83c\udfae Kernel-Range Explorer</h2><p>Visualize kernels and ranges of various linear transformations and understand their geometric meaning.</p>"
    },
    {
      "id": "rank-nullity-theorem",
      "title": "The Rank-Nullity Theorem: A Fundamental Relationship",
      "body": "<h1>Rank-Nullity Theorem: The Great Balance</h1><h2>Statement of the Theorem</h2><p>For any linear transformation T: V \u2192 W where V is finite-dimensional:</p><div class='theorem-box'><p><strong>dim(V) = dim(ker(T)) + dim(range(T))</strong></p></div><p>In matrix terms: <strong>n = nullity(A) + rank(A)</strong></p><h2>Intuitive Understanding</h2><p>The theorem says that the total 'dimension budget' of the domain is split between:</p><ul><li><strong>Kernel dimension:</strong> How much gets 'lost' (mapped to zero)</li><li><strong>Range dimension:</strong> How much 'survives' as output</li></ul><h2>Applications</h2><ol><li><strong>Existence of Solutions:</strong> Ax = b has solution \u27fa b \u2208 Col(A)</li><li><strong>Uniqueness:</strong> If solution exists, it's unique \u27fa Null(A) = {0}</li><li><strong>Invertibility:</strong> A is invertible \u27fa rank(A) = n</li></ol><h2>Examples</h2><div class='example-box'><p><strong>3\u00d74 matrix with rank 2:</strong></p><p>nullity = 4 - 2 = 2</p><p>So the null space is 2-dimensional (a plane through origin in \u211d\u2074)</p></div><h2>\ud83c\udfaf Dimension Detective</h2><p>Given partial information about a linear transformation, use the rank-nullity theorem to find missing dimensions.</p>"
    },
    {
      "id": "eigenvalues-definition",
      "title": "Eigenvalues and Eigenvectors: Special Directions",
      "body": "<h1>Eigenvalues and Eigenvectors: The Heart of Linear Algebra</h1><h2>Definition</h2><p>For square matrix A, a nonzero vector <strong>v</strong> is an <strong>eigenvector</strong> with <strong>eigenvalue</strong> \u03bb if:</p><div class='eigen-equation'><p><strong>Av = \u03bbv</strong></p></div><p>The transformation A simply scales the eigenvector <strong>v</strong> by the factor \u03bb.</p><h2>Geometric Interpretation</h2><ul><li><strong>Eigenvectors:</strong> Special directions that don't change under the transformation</li><li><strong>Eigenvalues:</strong> How much the eigenvectors get scaled</li><li><strong>\u03bb > 1:</strong> Vector gets stretched</li><li><strong>0 < \u03bb < 1:</strong> Vector gets shrunk</li><li><strong>\u03bb < 0:</strong> Vector gets reversed and scaled</li></ul><h2>The Characteristic Equation</h2><p>To find eigenvalues, solve:</p><p><strong>det(A - \u03bbI) = 0</strong></p><p>This polynomial in \u03bb is called the characteristic polynomial.</p><h2>Properties</h2><ul><li>An n\u00d7n matrix has at most n distinct eigenvalues</li><li>Eigenvectors corresponding to different eigenvalues are linearly independent</li><li>The sum of eigenvalues equals the trace of A</li><li>The product of eigenvalues equals the determinant of A</li></ul><h2>\ud83c\udfae Eigen-Explorer</h2><p>Visualize how eigenvectors behave under matrix transformations and find eigenvalues graphically.</p>"
    },
    {
      "id": "finding-eigenvalues",
      "title": "Finding Eigenvalues and Eigenvectors",
      "body": "<h1>The Eigenvalue-Eigenvector Algorithm</h1><h2>Step-by-Step Process</h2><ol><li><strong>Find eigenvalues:</strong> Solve det(A - \u03bbI) = 0</li><li><strong>For each eigenvalue \u03bb\u1d62:</strong> Solve (A - \u03bb\u1d62I)v = 0</li><li><strong>Find eigenspace:</strong> Null space of (A - \u03bb\u1d62I)</li><li><strong>Choose basis:</strong> Pick basis vectors for each eigenspace</li></ol><h2>Example Walkthrough</h2><div class='example-computation'><p><strong>Matrix:</strong> A = [3 1; 0 2]</p><p><strong>Step 1:</strong> det(A - \u03bbI) = det([3-\u03bb 1; 0 2-\u03bb]) = (3-\u03bb)(2-\u03bb) = 0</p><p><strong>Eigenvalues:</strong> \u03bb\u2081 = 3, \u03bb\u2082 = 2</p><p><strong>Step 2:</strong> For \u03bb\u2081 = 3: (A - 3I)v = 0 gives eigenvector [1; 0]</p><p><strong>Step 3:</strong> For \u03bb\u2082 = 2: (A - 2I)v = 0 gives eigenvector [1; -1]</p></div><h2>Special Cases</h2><ul><li><strong>Repeated eigenvalues:</strong> May have geometric multiplicity < algebraic multiplicity</li><li><strong>Complex eigenvalues:</strong> Occur in conjugate pairs for real matrices</li><li><strong>Zero eigenvalue:</strong> Matrix is not invertible</li></ul><h2>\ud83c\udfaf Eigenvalue Calculator</h2><p>Practice finding eigenvalues and eigenvectors for various matrices, including challenging cases.</p>"
    },
    {
      "id": "diagonalization",
      "title": "Diagonalization: Simplifying Matrix Powers",
      "body": "<h1>Diagonalization: The Ultimate Simplification</h1><h2>What is Diagonalization?</h2><p>A matrix A is <strong>diagonalizable</strong> if there exists an invertible matrix P such that:</p><p><strong>P\u207b\u00b9AP = D</strong></p><p>where D is a diagonal matrix.</p><h2>How to Diagonalize</h2><ol><li><strong>Find eigenvalues:</strong> Solve det(A - \u03bbI) = 0</li><li><strong>Find eigenvectors:</strong> For each eigenvalue, find basis for eigenspace</li><li><strong>Check diagonalizability:</strong> Need n linearly independent eigenvectors</li><li><strong>Form matrices:</strong> P = [v\u2081 v\u2082 ... v\u2099], D = diag(\u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2099)</li></ol><h2>When is Diagonalization Possible?</h2><ul><li><strong>Always possible:</strong> If A has n distinct eigenvalues</li><li><strong>Sometimes possible:</strong> If repeated eigenvalues have full geometric multiplicity</li><li><strong>Never possible:</strong> If geometric multiplicity < algebraic multiplicity for some eigenvalue</li></ul><h2>Applications</h2><ul><li><strong>Matrix powers:</strong> A\u207f = PD\u207fP\u207b\u00b9</li><li><strong>Systems of differential equations</strong></li><li><strong>Principal Component Analysis (PCA)</strong></li><li><strong>Google's PageRank algorithm</strong></li></ul><h2>\ud83c\udfae Diagonalization Workshop</h2><p>Practice diagonalizing matrices and compute large matrix powers efficiently.</p>"
    },
    {
      "id": "orthogonality",
      "title": "Orthogonality: When Vectors Meet at Right Angles",
      "body": "<h1>Orthogonality: The Foundation of Perpendicularity</h1><h2>Orthogonal Vectors</h2><p>Vectors <strong>u</strong> and <strong>v</strong> are <strong>orthogonal</strong> if:</p><p><strong>u \u00b7 v = 0</strong></p><h2>Orthogonal and Orthonormal Sets</h2><ul><li><strong>Orthogonal set:</strong> Vectors are mutually orthogonal</li><li><strong>Orthonormal set:</strong> Orthogonal + each vector has unit length</li></ul><h2>Gram-Schmidt Process</h2><p>Algorithm to convert any linearly independent set into an orthonormal set:</p><ol><li><strong>Normalize first vector:</strong> u\u2081 = v\u2081/||v\u2081||</li><li><strong>Project and subtract:</strong> For each subsequent vector, subtract its projections onto previous orthonormal vectors</li><li><strong>Normalize result</strong></li></ol><h2>Properties of Orthogonal Sets</h2><ul><li><strong>Automatic linear independence:</strong> Orthogonal vectors (except zero) are linearly independent</li><li><strong>Easy coordinates:</strong> In orthonormal basis, coordinates are just dot products</li><li><strong>Pythagorean theorem:</strong> ||u + v||\u00b2 = ||u||\u00b2 + ||v||\u00b2 when u \u22a5 v</li></ul><h2>\ud83c\udfaf Orthogonality Builder</h2><p>Use the Gram-Schmidt process to build orthonormal bases from given vectors.</p>"
    },
    {
      "id": "projections",
      "title": "Projections: Finding the Closest Point",
      "body": "<h1>Projections: Getting as Close as Possible</h1><h2>Vector Projection</h2><p>The projection of vector <strong>y</strong> onto vector <strong>u</strong> is:</p><div class='projection-formula'><p><strong>proj_u(y) = ((y \u00b7 u)/(u \u00b7 u)) u</strong></p></div><p>This gives the component of <strong>y</strong> in the direction of <strong>u</strong>.</p><h2>Projection onto Subspaces</h2><p>To project vector <strong>y</strong> onto subspace W with orthonormal basis {u\u2081, u\u2082, ..., u\u2096}:</p><p><strong>proj_W(y) = (y \u00b7 u\u2081)u\u2081 + (y \u00b7 u\u2082)u\u2082 + ... + (y \u00b7 u\u2096)u\u2096</strong></p><h2>Projection Matrix</h2><p>For subspace W = Col(A), the projection matrix is:</p><p><strong>P = A(A\u1d40A)\u207b\u00b9A\u1d40</strong></p><p>Then proj_W(y) = Py</p><h2>Properties</h2><ul><li><strong>Closest point:</strong> proj_W(y) is the closest point in W to y</li><li><strong>Orthogonal component:</strong> y - proj_W(y) \u22a5 W</li><li><strong>Idempotent:</strong> P\u00b2 = P (projecting twice = projecting once)</li></ul><h2>Applications</h2><ul><li><strong>Least squares:</strong> Finding best-fit lines</li><li><strong>Computer graphics:</strong> Shadow calculations</li><li><strong>Signal processing:</strong> Noise reduction</li></ul><h2>\ud83c\udfae Projection Visualizer</h2><p>Visualize projections onto lines and planes, and see how they minimize distances.</p>"
    },
    {
      "id": "orthogonal-complement",
      "title": "Orthogonal Complements: The Perpendicular Space",
      "body": "<h1>Orthogonal Complements: What's Left Perpendicular</h1><h2>Definition</h2><p>For subspace W of vector space V, the <strong>orthogonal complement</strong> W\u22a5 is:</p><p><strong>W\u22a5 = {v \u2208 V : v \u00b7 w = 0 for all w \u2208 W}</strong></p><h2>Key Properties</h2><ul><li><strong>W\u22a5 is a subspace</strong></li><li><strong>W \u2229 W\u22a5 = {0}</strong></li><li><strong>If V = \u211d\u207f, then dim(W) + dim(W\u22a5) = n</strong></li><li><strong>(W\u22a5)\u22a5 = W</strong> (for finite dimensions)</li></ul><h2>Finding Orthogonal Complements</h2><p>For W = span{v\u2081, v\u2082, ..., v\u2096} in \u211d\u207f:</p><ol><li>Form matrix A with rows v\u2081\u1d40, v\u2082\u1d40, ..., v\u2096\u1d40</li><li>W\u22a5 = Null(A)</li><li>Solve Ax = 0 to find basis for W\u22a5</li></ol><h2>Fundamental Subspaces</h2><p>For m\u00d7n matrix A:</p><ul><li><strong>Row(A)\u22a5 = Null(A)</strong></li><li><strong>Col(A)\u22a5 = Null(A\u1d40)</strong></li><li><strong>\u211d\u207f = Row(A) \u2295 Null(A)</strong></li><li><strong>\u211d\u1d50 = Col(A) \u2295 Null(A\u1d40)</strong></li></ul><h2>\ud83c\udfaf Complement Constructor</h2><p>Given a subspace, find its orthogonal complement and verify the fundamental relationships.</p>"
    },
    {
      "id": "data-science-connections",
      "title": "Linear Algebra in Data Science and Machine Learning",
      "body": "<h1>Linear Algebra: The Language of Data Science</h1><h2>\ud83e\udd16 Machine Learning Applications</h2><h3>Principal Component Analysis (PCA)</h3><ul><li><strong>Eigenvalue decomposition</strong> to find principal directions</li><li><strong>Dimensionality reduction</strong> while preserving variance</li><li><strong>Data visualization</strong> in lower dimensions</li></ul><h3>Linear Regression</h3><ul><li><strong>Normal equations:</strong> (X\u1d40X)\u207b\u00b9X\u1d40y</li><li><strong>Least squares</strong> as orthogonal projection</li><li><strong>Regularization</strong> using matrix norms</li></ul><h3>Neural Networks</h3><ul><li><strong>Forward propagation:</strong> Matrix multiplications</li><li><strong>Backpropagation:</strong> Chain rule with matrices</li><li><strong>Weight matrices</strong> as linear transformations</li></ul><h2>\ud83d\udcca Data Analysis Techniques</h2><ul><li><strong>Singular Value Decomposition (SVD):</strong> Matrix factorization for recommendation systems</li><li><strong>Correlation matrices:</strong> Understanding variable relationships</li><li><strong>Covariance analysis:</strong> Portfolio optimization</li></ul><h2>\ud83d\udd0d Real Project Examples</h2><div class='project-examples'><div class='project'><h3>Image Compression</h3><p>Use SVD to compress images by keeping only the largest singular values</p></div><div class='project'><h3>Recommendation Engine</h3><p>Matrix factorization to predict user preferences</p></div><div class='project'><h3>Face Recognition</h3><p>Eigenfaces using PCA for facial feature extraction</p></div></div><h2>\ud83c\udfae Interactive Lab</h2><p>Work with real datasets to see linear algebra concepts in action.</p>"
    },
    {
      "id": "engineering-applications",
      "title": "Linear Algebra in Engineering and Physics",
      "body": "<h1>Engineering Applications of Linear Algebra</h1><h2>\ud83c\udfd7\ufe0f Structural Engineering</h2><h3>Finite Element Analysis</h3><ul><li><strong>Stiffness matrices:</strong> Relating forces to displacements</li><li><strong>System of equations:</strong> Kd = F (stiffness \u00d7 displacement = force)</li><li><strong>Boundary conditions:</strong> Modifying matrices for constraints</li></ul><h3>Truss Analysis</h3><ul><li><strong>Equilibrium equations:</strong> Sum of forces = 0</li><li><strong>Matrix formulation:</strong> Large sparse systems</li><li><strong>Solution methods:</strong> Gaussian elimination for force distribution</li></ul><h2>\u26a1 Electrical Engineering</h2><h3>Circuit Analysis</h3><ul><li><strong>Kirchhoff's laws</strong> as linear systems</li><li><strong>Node voltage method</strong> using matrix equations</li><li><strong>AC analysis</strong> with complex matrices</li></ul><h3>Signal Processing</h3><ul><li><strong>Fourier transforms</strong> as matrix operations</li><li><strong>Filter design</strong> using eigenvalue analysis</li><li><strong>Digital signal processing</strong> algorithms</li></ul><h2>\ud83d\ude80 Control Systems</h2><ul><li><strong>State-space representation:</strong> \u1e8b = Ax + Bu</li><li><strong>Stability analysis</strong> using eigenvalues</li><li><strong>Controller design</strong> via pole placement</li></ul><h2>\ud83c\udfaf Engineering Challenge</h2><p>Design a simple truss structure and solve for member forces using matrix methods.</p>"
    },
    {
      "id": "svd-introduction",
      "title": "Introduction to Singular Value Decomposition",
      "body": "<h1>Singular Value Decomposition: The Ultimate Matrix Factorization</h1><h2>What is SVD?</h2><p>Every m\u00d7n matrix A can be factored as:</p><div class='svd-formula'><p><strong>A = U\u03a3V\u1d40</strong></p></div><p>where:</p><ul><li><strong>U:</strong> m\u00d7m orthogonal matrix (left singular vectors)</li><li><strong>\u03a3:</strong> m\u00d7n diagonal matrix (singular values)</li><li><strong>V:</strong> n\u00d7n orthogonal matrix (right singular vectors)</li></ul><h2>Geometric Interpretation</h2><p>SVD reveals that any linear transformation can be decomposed into:</p><ol><li><strong>Rotation/reflection</strong> (V \u1d40)</li><li><strong>Scaling along coordinate axes</strong> (\u03a3)</li><li><strong>Another rotation/reflection</strong> (U)</li></ol><h2>Key Properties</h2><ul><li><strong>Always exists</strong> for any matrix (unlike eigenvalue decomposition)</li><li><strong>Singular values are non-negative</strong> and arranged in decreasing order</li><li><strong>Rank equals number of non-zero singular values</strong></li></ul><h2>Applications</h2><ul><li><strong>Image compression:</strong> Keep only largest singular values</li><li><strong>Data analysis:</strong> Principal component analysis</li><li><strong>Recommendation systems:</strong> Matrix factorization</li><li><strong>Pseudoinverse:</strong> Moore-Penrose inverse for non-square matrices</li></ul><h2>\ud83c\udfae SVD Explorer</h2><p>Experiment with SVD on images and see how different numbers of singular values affect reconstruction quality.</p>"
    }
  ]
}