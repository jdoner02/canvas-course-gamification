{
  "pages": [
    {
      "id": "syllabus",
      "title": "Course Syllabus - MATH 231 Linear Algebra Mastery Journey",
      "body": "<h1>Welcome to Linear Algebra: A Mastery-Based Learning Adventure</h1><p>This course uses a <strong>skill tree approach</strong> where you'll progress through interconnected topics, unlocking new areas as you master prerequisites. Each module includes:</p><ul><li><strong>Mastery Checks:</strong> Demonstrate proficiency before advancing</li><li><strong>Badges & XP:</strong> Earn recognition for achievements</li><li><strong>Multiple Pathways:</strong> Choose your learning route based on interests and strengths</li><li><strong>Real-World Applications:</strong> See how linear algebra connects to your future career</li></ul><h2>Course Structure</h2><p>The course is organized into 12 interconnected modules, each building on previous knowledge. You must achieve at least 75% mastery on most modules to unlock the next level.</p><h2>Assessment Philosophy</h2><p>We focus on <em>mastery over speed</em>. You can retake assessments until you demonstrate proficiency. The goal is deep understanding, not just completion.</p><h2>Gamification Elements</h2><ul><li><strong>Experience Points (XP):</strong> Earn points for completing activities and demonstrating mastery</li><li><strong>Badges:</strong> Collect digital badges for specific achievements</li><li><strong>Leaderboards:</strong> Friendly competition to motivate progress</li><li><strong>Unlockable Content:</strong> Advanced topics become available as you progress</li></ul>"
    },
    {
      "id": "skill-tree-overview",
      "title": "Linear Algebra Skill Tree Overview",
      "body": "<h1>Your Learning Journey Map</h1><p>This interactive skill tree shows your path through Linear Algebra. Each node represents a key concept, and arrows show prerequisite relationships.</p><div class='skill-tree-diagram'><img src='/images/skill-tree-overview.png' alt='Linear Algebra Skill Tree' style='max-width: 100%; height: auto;'/></div><h2>Progression Levels</h2><ol><li><strong>Recognition:</strong> 'I know what this is' - Identify and recall concepts</li><li><strong>Application:</strong> 'I can use this' - Apply procedures in guided problems</li><li><strong>Intuition:</strong> 'I understand why' - Explain reasoning and connections</li><li><strong>Synthesis:</strong> 'I can connect and innovate' - Combine concepts creatively</li><li><strong>Mastery:</strong> 'I can teach this' - Expert-level understanding and application</li></ol><h2>Your Current Progress</h2><p><em>Your personalized progress will be displayed here as you advance through the course.</em></p>"
    },
    {
      "id": "prerequisite-review",
      "title": "Prerequisite Review - Mathematical Foundations",
      "body": "<h1>Essential Mathematical Background</h1><p>Before diving into linear algebra, let's review the key mathematical concepts you'll need.</p><h2>Arithmetic & Basic Algebra</h2><ul><li>Real number operations (addition, multiplication, fractions)</li><li>Solving linear equations: ax + b = 0</li><li>Algebraic manipulation and simplification</li><li>Working with negative numbers and exponents</li></ul><h2>Coordinate Geometry</h2><ul><li>Cartesian coordinate system</li><li>Distance formula: ‚àö[(x‚ÇÇ-x‚ÇÅ)¬≤ + (y‚ÇÇ-y‚ÇÅ)¬≤]</li><li>Slope and intercepts of lines</li><li>Basic graphing skills</li></ul><h2>Functions & Basic Vectors</h2><ul><li>Function notation and evaluation</li><li>Linear and quadratic functions</li><li>Introduction to vectors as ordered pairs (x,y)</li><li>Basic vector addition and scalar multiplication</li></ul><h2>Self-Assessment</h2><p>Complete the prerequisite assessment quiz to identify any areas that need review. Don't worry - we'll provide resources to strengthen any weak areas!</p>"
    },
    {
      "id": "gamification-guide",
      "title": "How to Navigate Your Gamified Learning Experience",
      "body": "<h1>Your Gaming Guide to Linear Algebra</h1><p>This course transforms traditional math learning into an engaging adventure. Here's how to make the most of the gamification features:</p><h2>üéÆ Core Game Mechanics</h2><h3>Experience Points (XP)</h3><ul><li>Earn XP for completing activities, mastering concepts, and helping classmates</li><li>Track your total XP on the course dashboard</li><li>XP accumulates to unlock special content and privileges</li></ul><h3>üèÜ Badges & Achievements</h3><ul><li><strong>Skill Badges:</strong> Master specific concepts (e.g., 'Vector Warrior', 'Matrix Manipulator')</li><li><strong>Behavior Badges:</strong> Demonstrate good learning habits (e.g., 'Early Bird', 'Help Hero')</li><li><strong>Milestone Badges:</strong> Complete major course sections (e.g., 'Foundation Explorer', 'Transformation Mage')</li></ul><h3>üó∫Ô∏è Mastery Paths</h3><p>Based on your performance, Canvas will automatically guide you to personalized content:</p><ul><li><strong>Express Path:</strong> For those demonstrating strong understanding</li><li><strong>Standard Path:</strong> Balanced progression with practice</li><li><strong>Support Path:</strong> Additional resources and guided practice</li></ul><h2>üìä Progress Tracking</h2><ul><li>Module completion percentages</li><li>Concept mastery indicators</li><li>XP leaderboard (opt-in participation)</li><li>Personal achievement gallery</li></ul><h2>üéØ Tips for Success</h2><ol><li><strong>Aim for Mastery:</strong> 75%+ scores unlock the next level</li><li><strong>Use Retakes:</strong> Keep trying until you achieve mastery</li><li><strong>Explore Bonus Content:</strong> Additional XP and deeper understanding</li><li><strong>Collaborate:</strong> Help others to earn collaboration badges</li></ol>"
    },
    {
      "id": "vectors-definition",
      "title": "What is a Vector? - Building Block of Linear Algebra",
      "body": "<h1>Vectors: Your First Linear Algebra Concept</h1><h2>Definition and Notation</h2><p>A <strong>vector</strong> is an ordered list of numbers that represents both magnitude (size) and direction. In ‚Ñù‚Åø, a vector is an n-tuple of real numbers.</p><div class='math-example'><p>Examples:</p><ul><li>In ‚Ñù¬≤: <strong>v</strong> = (3, 4) or <strong>v</strong> = [3, 4]·µÄ</li><li>In ‚Ñù¬≥: <strong>u</strong> = (1, -2, 5)</li><li>In ‚Ñù‚Åø: <strong>w</strong> = (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)</li></ul></div><h2>Geometric Interpretation</h2><p>In 2D and 3D, vectors can be visualized as arrows:</p><ul><li><strong>Magnitude:</strong> Length of the arrow</li><li><strong>Direction:</strong> Where the arrow points</li><li><strong>Position Independence:</strong> Vectors can be moved without changing their identity</li></ul><h2>Why Vectors Matter</h2><p>Vectors appear everywhere in science and technology:</p><ul><li><strong>Physics:</strong> Force, velocity, acceleration</li><li><strong>Computer Graphics:</strong> Position, rotation, scaling</li><li><strong>Data Science:</strong> Feature vectors, data points</li><li><strong>Engineering:</strong> Loads, displacements, fields</li></ul><h2>üéØ Learning Checkpoint</h2><p>Can you identify which of these represent vectors and explain why?</p><ol><li>Temperature in a room</li><li>Wind velocity</li><li>Your position on a map</li><li>The number 42</li></ol>"
    },
    {
      "id": "vector-operations",
      "title": "Vector Operations - Addition, Subtraction, and Scalar Multiplication",
      "body": "<h1>Essential Vector Operations</h1><h2>Vector Addition</h2><p>Add vectors component-wise: <strong>u</strong> + <strong>v</strong> = (u‚ÇÅ + v‚ÇÅ, u‚ÇÇ + v‚ÇÇ, ..., u‚Çô + v‚Çô)</p><div class='interactive-example'><p><strong>Example:</strong> (2, 3) + (1, -4) = (3, -1)</p><p><strong>Geometric Interpretation:</strong> Place vectors tip-to-tail or use the parallelogram rule</p></div><h2>Scalar Multiplication</h2><p>Multiply each component by a scalar: c<strong>v</strong> = (cv‚ÇÅ, cv‚ÇÇ, ..., cv‚Çô)</p><ul><li><strong>c > 1:</strong> Stretches the vector</li><li><strong>0 < c < 1:</strong> Shrinks the vector</li><li><strong>c < 0:</strong> Reverses direction and scales</li><li><strong>c = 0:</strong> Results in the zero vector</li></ul><h2>Properties of Vector Operations</h2><div class='property-box'><h3>Addition Properties:</h3><ul><li><strong>Commutative:</strong> <strong>u</strong> + <strong>v</strong> = <strong>v</strong> + <strong>u</strong></li><li><strong>Associative:</strong> (<strong>u</strong> + <strong>v</strong>) + <strong>w</strong> = <strong>u</strong> + (<strong>v</strong> + <strong>w</strong>)</li><li><strong>Zero Vector:</strong> <strong>v</strong> + <strong>0</strong> = <strong>v</strong></li><li><strong>Additive Inverse:</strong> <strong>v</strong> + (-<strong>v</strong>) = <strong>0</strong></li></ul></div><h2>üéÆ Practice Challenge</h2><p>Try these vector operations and check your geometric intuition:</p><ol><li>Calculate: 3(2, -1) + 2(1, 3)</li><li>Find a vector that when added to (4, 1) gives (0, 5)</li><li>What happens geometrically when you multiply a vector by -1?</li></ol>"
    },
    {
      "id": "dot-product-geometry",
      "title": "The Dot Product - Measuring Angles and Projections",
      "body": "<h1>The Dot Product: Gateway to Geometry</h1><h2>Definition</h2><p>The dot product of two vectors <strong>u</strong> and <strong>v</strong> in ‚Ñù‚Åø is:</p><div class='formula-box'><strong>u</strong> ¬∑ <strong>v</strong> = u‚ÇÅv‚ÇÅ + u‚ÇÇv‚ÇÇ + ... + u‚Çôv‚Çô</div><h2>Geometric Formula</h2><p>The dot product relates to the angle Œ∏ between vectors:</p><div class='formula-box'><strong>u</strong> ¬∑ <strong>v</strong> = ||<strong>u</strong>|| ||<strong>v</strong>|| cos Œ∏</div><p>Where ||<strong>v</strong>|| = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤) is the magnitude (length) of vector <strong>v</strong>.</p><h2>Key Properties</h2><ul><li><strong>Orthogonality:</strong> <strong>u</strong> ¬∑ <strong>v</strong> = 0 ‚ü∫ <strong>u</strong> ‚ä• <strong>v</strong> (vectors are perpendicular)</li><li><strong>Magnitude:</strong> <strong>v</strong> ¬∑ <strong>v</strong> = ||<strong>v</strong>||¬≤</li><li><strong>Angle Detection:</strong> cos Œ∏ = (<strong>u</strong> ¬∑ <strong>v</strong>) / (||<strong>u</strong>|| ||<strong>v</strong>||)</li></ul><h2>Applications</h2><div class='application-grid'><div class='app-card'><h3>Physics</h3><p>Work = Force ¬∑ Displacement</p></div><div class='app-card'><h3>Computer Graphics</h3><p>Lighting calculations, surface normals</p></div><div class='app-card'><h3>Data Science</h3><p>Similarity measures, correlation</p></div></div><h2>üéØ Mastery Check</h2><p>Given vectors <strong>a</strong> = (3, 4) and <strong>b</strong> = (1, 2):</p><ol><li>Calculate <strong>a</strong> ¬∑ <strong>b</strong></li><li>Find the angle between them</li><li>Determine if they are orthogonal</li></ol>"
    },
    {
      "id": "linear-combinations",
      "title": "Linear Combinations - Building New Vectors",
      "body": "<h1>Linear Combinations: The Foundation of Vector Spaces</h1><h2>Definition</h2><p>A <strong>linear combination</strong> of vectors v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ is any vector of the form:</p><div class='formula-box'>c‚ÇÅ<strong>v‚ÇÅ</strong> + c‚ÇÇ<strong>v‚ÇÇ</strong> + ... + c‚Çñ<strong>v‚Çñ</strong></div><p>where c‚ÇÅ, c‚ÇÇ, ..., c‚Çñ are scalars (real numbers).</p><h2>Examples</h2><div class='example-box'><p><strong>Given:</strong> <strong>v‚ÇÅ</strong> = (1, 0) and <strong>v‚ÇÇ</strong> = (0, 1)</p><p><strong>Linear combinations include:</strong></p><ul><li>2<strong>v‚ÇÅ</strong> + 3<strong>v‚ÇÇ</strong> = 2(1, 0) + 3(0, 1) = (2, 3)</li><li>-<strong>v‚ÇÅ</strong> + 0.5<strong>v‚ÇÇ</strong> = (-1, 0.5)</li><li>œÄ<strong>v‚ÇÅ</strong> + ‚àö2<strong>v‚ÇÇ</strong> = (œÄ, ‚àö2)</li></ul></div><h2>Geometric Interpretation</h2><p>In ‚Ñù¬≤:</p><ul><li><strong>One vector:</strong> Forms a line through the origin</li><li><strong>Two non-parallel vectors:</strong> Fill the entire plane</li><li><strong>Two parallel vectors:</strong> Still just a line</li></ul><h2>Key Questions</h2><ol><li><strong>Can we express a target vector as a linear combination?</strong></li><li><strong>What's the smallest set of vectors needed?</strong></li><li><strong>When do we get 'redundant' vectors?</strong></li></ol><h2>üéÆ Interactive Challenge</h2><p>Express the vector (5, 2) as a linear combination of:</p><ul><li><strong>v‚ÇÅ</strong> = (1, 1) and <strong>v‚ÇÇ</strong> = (1, -1)</li></ul><p><em>Hint: Set up the equation c‚ÇÅ(1, 1) + c‚ÇÇ(1, -1) = (5, 2) and solve!</em></p>"
    },
    {
      "id": "span-concept",
      "title": "Span - The Set of All Possible Linear Combinations",
      "body": "<h1>Span: Exploring All Possibilities</h1><h2>Definition</h2><p>The <strong>span</strong> of a set of vectors {v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ} is the set of ALL possible linear combinations of these vectors.</p><div class='formula-box'>Span{v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ} = {c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ : c‚ÇÅ, c‚ÇÇ, ..., c‚Çñ ‚àà ‚Ñù}</div><h2>Geometric Visualization</h2><table class='span-table'><tr><th>Number of Vectors</th><th>Typical Span in ‚Ñù¬≥</th><th>Dimension</th></tr><tr><td>1 non-zero vector</td><td>Line through origin</td><td>1</td></tr><tr><td>2 linearly independent</td><td>Plane through origin</td><td>2</td></tr><tr><td>3 linearly independent</td><td>All of ‚Ñù¬≥</td><td>3</td></tr></table><h2>Important Properties</h2><ul><li><strong>Always includes the zero vector:</strong> Set all coefficients to 0</li><li><strong>Closed under addition:</strong> Sum of vectors in span is also in span</li><li><strong>Closed under scalar multiplication:</strong> Scalar multiple of vector in span is also in span</li></ul><h2>Questions to Ask</h2><ol><li><strong>Does vector <strong>w</strong> lie in Span{v‚ÇÅ, v‚ÇÇ}?</strong> ‚üπ Solve c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ = w</li><li><strong>Do these vectors span all of ‚Ñù‚Åø?</strong> ‚üπ Check if we can express any vector</li><li><strong>What's the 'shape' of this span?</strong> ‚üπ Line, plane, or higher dimension?</li></ol><h2>üéØ Challenge Problem</h2><p>Determine if the vector (1, 2, 3) is in the span of:</p><ul><li><strong>v‚ÇÅ</strong> = (1, 0, 1)</li><li><strong>v‚ÇÇ</strong> = (0, 1, 1)</li><li><strong>v‚ÇÉ</strong> = (1, 1, 0)</li></ul><p><strong>Method:</strong> Set up the linear system and solve using techniques from the next module!</p>"
    },
    {
      "id": "linear-systems-intro",
      "title": "Systems of Linear Equations - Setting Up the Problem",
      "body": "<h1>Linear Systems: Where Algebra Meets Geometry</h1><h2>What is a Linear System?</h2><p>A system of linear equations is a collection of linear equations involving the same variables.</p><div class='system-example'><h3>Example System:</h3><p>x + 2y - z = 3</p><p>2x - y + z = 0</p><p>-x + y + 2z = -1</p></div><h2>Matrix Form: Ax = b</h2><p>We can represent this system using matrices:</p><div class='matrix-representation'><p><strong>Coefficient Matrix A:</strong> Contains the coefficients</p><p><strong>Variable Vector x:</strong> Contains the unknowns</p><p><strong>Constant Vector b:</strong> Contains the right-hand sides</p></div><h2>Types of Solutions</h2><ol><li><strong>Unique Solution:</strong> Lines/planes intersect at exactly one point</li><li><strong>No Solution:</strong> Inconsistent system (parallel lines/planes)</li><li><strong>Infinite Solutions:</strong> Lines/planes coincide or intersect along a line/plane</li></ol><h2>Geometric Interpretation</h2><div class='geometry-grid'><div class='geo-card'><h3>2 Variables</h3><p>Each equation represents a line. Solution is intersection point(s).</p></div><div class='geo-card'><h3>3 Variables</h3><p>Each equation represents a plane. Solution is intersection of all planes.</p></div><div class='geo-card'><h3>n Variables</h3><p>Each equation represents a hyperplane in ‚Ñù‚Åø.</p></div></div><h2>üéÆ Real-World Connection</h2><p><strong>Traffic Flow Problem:</strong> Model traffic through intersections using linear systems where variables represent flow rates and equations represent conservation of vehicles.</p><h2>üéØ Check Your Understanding</h2><p>Write the following system in matrix form Ax = b:</p><p>3x - y = 7</p><p>x + 2y = 1</p>"
    },
    {
      "id": "gaussian-elimination",
      "title": "Gaussian Elimination - The Systematic Solution Method",
      "body": "<h1>Gaussian Elimination: The Universal Solver</h1><h2>The Algorithm</h2><p>Gaussian Elimination uses three <strong>elementary row operations</strong> to systematically solve linear systems:</p><ol><li><strong>Row Swap:</strong> R·µ¢ ‚Üî R‚±º</li><li><strong>Row Scaling:</strong> cR·µ¢ ‚Üí R·µ¢ (c ‚â† 0)</li><li><strong>Row Addition:</strong> R·µ¢ + cR‚±º ‚Üí R·µ¢</li></ol><h2>Step-by-Step Process</h2><div class='algorithm-steps'><h3>Goal: Transform to Row Echelon Form (REF)</h3><ol><li><strong>Create leading 1:</strong> Make first nonzero entry in each row equal to 1</li><li><strong>Create zeros below:</strong> Eliminate entries below each leading 1</li><li><strong>Move right:</strong> Work column by column from left to right</li></ol></div><h2>Example Walkthrough</h2><div class='elimination-example'><h3>Solve:</h3><table><tr><td>x + 2y - z = 3</td><td rowspan='3'>‚Üí</td><td>Augmented Matrix:</td></tr><tr><td>2x - y + z = 0</td><td>[1  2 -1 |  3]</td></tr><tr><td>-x + y + 2z = -1</td><td>[2 -1  1 |  0]</td></tr><tr><td></td><td></td><td>[-1  1  2 | -1]</td></tr></table></div><h2>Row Echelon Form (REF)</h2><ul><li>All nonzero rows above rows of all zeros</li><li>Leading entry of each row is to the right of leading entry above it</li><li>All entries below a leading entry are zero</li></ul><h2>Reduced Row Echelon Form (RREF)</h2><ul><li>Satisfies REF conditions</li><li>Leading entry in each row is 1</li><li>All other entries in columns with leading 1s are zero</li></ul><h2>üéÆ Practice Game</h2><p>Transform this augmented matrix to RREF:</p><p>[2  4  2 |  8]</p><p>[1  1  3 |  5]</p><p>[3  5  1 | 11]</p><p><em>Track your moves and try to minimize the number of operations!</em></p>"
    },
    {
      "id": "applications-overview",
      "title": "Linear Algebra in the Real World",
      "body": "<h1>Where Linear Algebra Powers Our World</h1><h2>üéÆ Computer Graphics & Gaming</h2><ul><li><strong>3D Transformations:</strong> Rotation, scaling, translation matrices</li><li><strong>Computer Vision:</strong> Image processing and object recognition</li><li><strong>Animation:</strong> Skeletal animation using transformation matrices</li><li><strong>Virtual Reality:</strong> Real-time 3D rendering</li></ul><h2>üìä Data Science & Machine Learning</h2><ul><li><strong>Principal Component Analysis (PCA):</strong> Dimensionality reduction using eigenvalues</li><li><strong>Linear Regression:</strong> Finding best-fit lines using normal equations</li><li><strong>Neural Networks:</strong> Matrix operations in forward/backward propagation</li><li><strong>Recommendation Systems:</strong> Matrix factorization techniques</li></ul><h2>üèóÔ∏è Engineering Applications</h2><ul><li><strong>Structural Analysis:</strong> Solving systems for forces and displacements</li><li><strong>Circuit Analysis:</strong> Kirchhoff's laws as linear systems</li><li><strong>Control Systems:</strong> State-space representations</li><li><strong>Signal Processing:</strong> Fourier transforms and filtering</li></ul><h2>üí∞ Business & Economics</h2><ul><li><strong>Supply Chain Optimization:</strong> Linear programming</li><li><strong>Portfolio Optimization:</strong> Risk-return analysis using covariance matrices</li><li><strong>Market Analysis:</strong> Factor models and correlation analysis</li></ul><h2>üîí Cybersecurity & Cryptography</h2><ul><li><strong>Error-Correcting Codes:</strong> Vector spaces over finite fields</li><li><strong>Hill Cipher:</strong> Matrix-based encryption</li><li><strong>Network Security:</strong> Graph theory and adjacency matrices</li></ul><h2>üéØ Career Connections</h2><p>Select your career interest to see specific linear algebra applications:</p><div class='career-selector'><button>Software Engineer</button><button>Data Scientist</button><button>Mechanical Engineer</button><button>Financial Analyst</button><button>Game Developer</button></div>"
    },
    {
      "id": "vectors-applications",
      "title": "Vector Applications in Science and Technology",
      "body": "<h1>Vectors in Action</h1><h2>üöÄ Physics Applications</h2><ul><li><strong>Force Vectors:</strong> Combining multiple forces acting on an object</li><li><strong>Velocity and Acceleration:</strong> Motion in multiple dimensions</li><li><strong>Electric and Magnetic Fields:</strong> Vector field representations</li></ul><h2>üéÆ Computer Graphics</h2><ul><li><strong>Position Vectors:</strong> Locating objects in 3D space</li><li><strong>Normal Vectors:</strong> Surface lighting calculations</li><li><strong>Direction Vectors:</strong> Camera orientation and movement</li></ul><h2>üìä Data Science</h2><ul><li><strong>Feature Vectors:</strong> Representing data points in machine learning</li><li><strong>Word Embeddings:</strong> Representing words as high-dimensional vectors</li><li><strong>Principal Components:</strong> Dimensionality reduction directions</li></ul><h2>üéØ Interactive Examples</h2><p>Explore how vectors are used in real-world scenarios through interactive simulations and practice problems.</p>"
    },
    {
      "id": "subspace-introduction",
      "title": "Introduction to Subspaces",
      "body": "<h1>Subspaces: Vector Spaces Within Vector Spaces</h1><h2>What is a Subspace?</h2><p>A subspace of a vector space V is a subset H of V that is itself a vector space under the same operations.</p><h2>The Subspace Test</h2><p>A subset H of vector space V is a subspace if and only if:</p><ol><li><strong>Zero vector:</strong> <strong>0</strong> ‚àà H</li><li><strong>Closure under addition:</strong> If <strong>u</strong>, <strong>v</strong> ‚àà H, then <strong>u</strong> + <strong>v</strong> ‚àà H</li><li><strong>Closure under scalar multiplication:</strong> If <strong>v</strong> ‚àà H and c is a scalar, then c<strong>v</strong> ‚àà H</li></ol><h2>Common Examples</h2><ul><li><strong>The span of any set of vectors</strong> is always a subspace</li><li><strong>Lines through the origin</strong> in ‚Ñù¬≤</li><li><strong>Planes through the origin</strong> in ‚Ñù¬≥</li><li><strong>The null space</strong> of a matrix</li></ul><h2>üéØ Practice</h2><p>Determine if these sets are subspaces of ‚Ñù¬≤:</p><ol><li>All vectors of the form (a, 2a)</li><li>All vectors (x, y) where x ‚â• 0</li><li>All vectors (x, y) where x + y = 1</li></ol>"
    },
    {
      "id": "augmented-matrices",
      "title": "Augmented Matrices and Row Operations",
      "body": "<h1>Augmented Matrices: Organizing Linear Systems</h1><h2>What is an Augmented Matrix?</h2><p>An augmented matrix combines the coefficient matrix and the constant vector into one rectangular array:</p><div class='matrix-example'><p>System: ax + by = e, cx + dy = f</p><p>Augmented Matrix: [a b | e]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[c d | f]</p></div><h2>Elementary Row Operations</h2><ol><li><strong>Row Swap:</strong> R‚ÇÅ ‚Üî R‚ÇÇ</li><li><strong>Row Multiplication:</strong> kR‚ÇÅ ‚Üí R‚ÇÅ (k ‚â† 0)</li><li><strong>Row Addition:</strong> R‚ÇÅ + kR‚ÇÇ ‚Üí R‚ÇÅ</li></ol><h2>Why These Operations Work</h2><p>Each row operation corresponds to a valid manipulation of the original system of equations that preserves the solution set.</p><h2>üéÆ Interactive Practice</h2><p>Practice performing row operations on augmented matrices to solve systems step by step.</p>"
    },
    {
      "id": "solution-interpretation",
      "title": "Interpreting Solutions of Linear Systems",
      "body": "<h1>Understanding What Solutions Tell Us</h1><h2>Types of Solution Sets</h2><ol><li><strong>Unique Solution:</strong> Exactly one solution vector</li><li><strong>No Solution:</strong> Inconsistent system (0 = 1 appears in RREF)</li><li><strong>Infinite Solutions:</strong> Free variables create a parametric solution</li></ol><h2>Parametric Solutions</h2><p>When there are free variables, express the solution in parametric form:</p><div class='parametric-example'><p>If x‚ÇÉ is free, then:</p><p>x‚ÇÅ = 2 - 3x‚ÇÉ</p><p>x‚ÇÇ = 1 + x‚ÇÉ</p><p>x‚ÇÉ = t (parameter)</p></div><h2>Geometric Interpretation</h2><ul><li><strong>2 equations, 2 unknowns:</strong> Lines intersecting at a point, parallel, or coincident</li><li><strong>3 equations, 3 unknowns:</strong> Planes intersecting at a point, along a line, or nowhere</li></ul><h2>üéØ Solution Analysis</h2><p>Given RREF form, determine the type of solution and express it appropriately.</p>"
    },
    {
      "id": "matrix-definition",
      "title": "What is a Matrix? Definition and Notation",
      "body": "<h1>Matrices: Rectangular Arrays of Numbers</h1><h2>Definition</h2><p>A matrix is a rectangular array of numbers arranged in rows and columns. An m√ón matrix has m rows and n columns.</p><h2>Matrix Notation</h2><div class='matrix-notation'><p>General m√ón matrix A:</p><p>A = [a‚ÇÅ‚ÇÅ a‚ÇÅ‚ÇÇ ‚ãØ a‚ÇÅ‚Çô]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[a‚ÇÇ‚ÇÅ a‚ÇÇ‚ÇÇ ‚ãØ a‚ÇÇ‚Çô]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[‚ãÆ&nbsp;&nbsp;&nbsp;&nbsp;‚ãÆ&nbsp;&nbsp;&nbsp;&nbsp;‚ã±&nbsp;&nbsp;&nbsp;‚ãÆ&nbsp;&nbsp;]</p><p>&nbsp;&nbsp;&nbsp;&nbsp;[a‚Çò‚ÇÅ a‚Çò‚ÇÇ ‚ãØ a‚Çò‚Çô]</p></div><h2>Special Types of Matrices</h2><ul><li><strong>Square Matrix:</strong> m = n (same number of rows and columns)</li><li><strong>Column Vector:</strong> n = 1 (single column)</li><li><strong>Row Vector:</strong> m = 1 (single row)</li><li><strong>Zero Matrix:</strong> All entries are zero</li></ul><h2>Matrix Equality</h2><p>Two matrices are equal if they have the same dimensions and corresponding entries are equal.</p><h2>üéÆ Recognition Game</h2><p>Identify the dimensions and special properties of various matrices presented in random order.</p>"
    },
    {
      "id": "matrix-operations",
      "title": "Matrix Operations: Addition, Subtraction, and Scalar Multiplication",
      "body": "<h1>Basic Matrix Operations</h1><h2>Matrix Addition and Subtraction</h2><p>Matrices can be added or subtracted if they have the same dimensions. Operations are performed element-wise:</p><div class='matrix-operation'><p>(A + B)·µ¢‚±º = A·µ¢‚±º + B·µ¢‚±º</p><p>(A - B)·µ¢‚±º = A·µ¢‚±º - B·µ¢‚±º</p></div><h2>Scalar Multiplication</h2><p>Multiply each entry of the matrix by the scalar:</p><div class='scalar-mult'><p>(cA)·µ¢‚±º = c¬∑A·µ¢‚±º</p></div><h2>Properties</h2><ul><li><strong>Commutative:</strong> A + B = B + A</li><li><strong>Associative:</strong> (A + B) + C = A + (B + C)</li><li><strong>Distributive:</strong> c(A + B) = cA + cB</li><li><strong>Zero Matrix:</strong> A + O = A</li></ul><h2>üéØ Practice Problems</h2><p>Perform various matrix operations and verify the properties with specific examples.</p>"
    },
    {
      "id": "matrix-properties",
      "title": "Matrix Properties and Special Matrices",
      "body": "<h1>Important Matrix Properties and Types</h1><h2>Special Matrices</h2><ul><li><strong>Identity Matrix I:</strong> Square matrix with 1s on diagonal, 0s elsewhere</li><li><strong>Diagonal Matrix:</strong> Non-zero entries only on the main diagonal</li><li><strong>Triangular Matrices:</strong> Upper or lower triangular</li><li><strong>Symmetric Matrix:</strong> A = A·µÄ (equal to its transpose)</li></ul><h2>Matrix Transpose</h2><p>The transpose A·µÄ of matrix A is obtained by swapping rows and columns:</p><p>(A·µÄ)·µ¢‚±º = A‚±º·µ¢</p><h2>Transpose Properties</h2><ul><li>(A·µÄ)·µÄ = A</li><li>(A + B)·µÄ = A·µÄ + B·µÄ</li><li>(cA)·µÄ = cA·µÄ</li><li>(AB)·µÄ = B·µÄA·µÄ</li></ul><h2>üéÆ Matrix Type Detective</h2><p>Given various matrices, identify their special properties and classify them.</p>"
    },
    {
      "id": "linear-independence",
      "title": "Linear Independence: When Vectors Don't Depend on Each Other",
      "body": "<h1>Linear Independence: The Key to Efficiency</h1><h2>Definition</h2><p>Vectors v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ are <strong>linearly independent</strong> if the only solution to:</p><p>c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ = <strong>0</strong></p><p>is c‚ÇÅ = c‚ÇÇ = ... = c‚Çñ = 0.</p><h2>Linear Dependence</h2><p>Vectors are <strong>linearly dependent</strong> if there exist scalars (not all zero) such that the linear combination equals zero. This means one vector can be written as a linear combination of the others.</p><h2>Geometric Interpretation</h2><ul><li><strong>2 vectors in ‚Ñù¬≤:</strong> Independent if not parallel</li><li><strong>3 vectors in ‚Ñù¬≥:</strong> Independent if not coplanar</li><li><strong>More than n vectors in ‚Ñù‚Åø:</strong> Always dependent</li></ul><h2>Testing for Independence</h2><ol><li>Set up the equation c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çñv‚Çñ = 0</li><li>Write as a homogeneous system</li><li>Find RREF of coefficient matrix</li><li>Independent ‚ü∫ only trivial solution</li></ol><h2>üéØ Independence Detective</h2><p>Determine whether given sets of vectors are linearly independent or dependent.</p>"
    },
    {
      "id": "basis-definition",
      "title": "Basis: The Minimal Spanning Set",
      "body": "<h1>Basis: Efficient Building Blocks for Vector Spaces</h1><h2>Definition</h2><p>A <strong>basis</strong> for a vector space V is a set of vectors that:</p><ol><li><strong>Spans V:</strong> Every vector in V can be expressed as a linear combination</li><li><strong>Is linearly independent:</strong> No vector is redundant</li></ol><h2>Key Properties</h2><ul><li><strong>Uniqueness of Representation:</strong> Every vector has a unique representation in terms of basis vectors</li><li><strong>Same Size:</strong> All bases for the same vector space have the same number of vectors</li><li><strong>Efficiency:</strong> A basis is the smallest set that spans the space</li></ul><h2>Standard Bases</h2><div class='standard-bases'><p><strong>‚Ñù¬≤:</strong> {(1,0), (0,1)} = {e‚ÇÅ, e‚ÇÇ}</p><p><strong>‚Ñù¬≥:</strong> {(1,0,0), (0,1,0), (0,0,1)} = {e‚ÇÅ, e‚ÇÇ, e‚ÇÉ}</p><p><strong>‚Ñù‚Åø:</strong> {e‚ÇÅ, e‚ÇÇ, ..., e‚Çô} where e·µ¢ has 1 in position i, 0s elsewhere</p></div><h2>Finding a Basis</h2><ol><li>Start with a spanning set</li><li>Remove dependent vectors one by one</li><li>What remains is a basis</li></ol><h2>üéÆ Basis Builder Challenge</h2><p>Given a set of vectors, determine if they form a basis, and if not, modify the set to create one.</p>"
    },
    {
      "id": "dimension-coordinates",
      "title": "Dimension and Coordinate Systems",
      "body": "<h1>Dimension: Measuring the Size of Vector Spaces</h1><h2>Definition of Dimension</h2><p>The <strong>dimension</strong> of a vector space V is the number of vectors in any basis for V.</p><h2>Coordinate Representation</h2><p>Given a basis B = {b‚ÇÅ, b‚ÇÇ, ..., b‚Çô} for vector space V, every vector v ‚àà V can be uniquely written as:</p><p>v = c‚ÇÅb‚ÇÅ + c‚ÇÇb‚ÇÇ + ... + c‚Çôb‚Çô</p><p>The scalars (c‚ÇÅ, c‚ÇÇ, ..., c‚Çô) are the <strong>coordinates of v relative to basis B</strong>.</p><h2>Change of Basis</h2><p>Different bases give different coordinate representations for the same vector. The change of basis matrix converts coordinates between different bases.</p><h2>Dimension Facts</h2><ul><li><strong>‚Ñù‚Åø has dimension n</strong></li><li><strong>A line through origin has dimension 1</strong></li><li><strong>A plane through origin has dimension 2</strong></li><li><strong>The zero space {0} has dimension 0</strong></li></ul><h2>üéØ Coordinate Conversion</h2><p>Practice converting vectors between different coordinate systems and bases.</p>"
    },
    {
      "id": "matrix-inverse",
      "title": "Matrix Inverses: Undoing Matrix Operations",
      "body": "<h1>Matrix Inverses: The Multiplicative 'Undo' Operation</h1><h2>Definition</h2><p>For a square matrix A, the inverse A‚Åª¬π (if it exists) satisfies:</p><p>AA‚Åª¬π = A‚Åª¬πA = I</p><p>where I is the identity matrix.</p><h2>When Does an Inverse Exist?</h2><p>A square matrix A is <strong>invertible</strong> (or <strong>nonsingular</strong>) if and only if:</p><ul><li>det(A) ‚â† 0</li><li>A has full rank</li><li>The columns of A are linearly independent</li><li>Ax = 0 has only the trivial solution</li></ul><h2>Finding Inverses</h2><h3>2√ó2 Case:</h3><p>For A = [a b; c d], if ad - bc ‚â† 0:</p><p>A‚Åª¬π = (1/(ad-bc)) [d -b; -c a]</p><h3>General Case: Gauss-Jordan Method</h3><ol><li>Form augmented matrix [A | I]</li><li>Row reduce to [I | A‚Åª¬π]</li><li>If impossible to reach [I | ?], then A is not invertible</li></ol><h2>Properties of Inverses</h2><ul><li>(A‚Åª¬π)‚Åª¬π = A</li><li>(AB)‚Åª¬π = B‚Åª¬πA‚Åª¬π</li><li>(A·µÄ)‚Åª¬π = (A‚Åª¬π)·µÄ</li></ul><h2>üéÆ Inverse Calculator Challenge</h2><p>Find inverses of various matrices using different methods and verify your results.</p>"
    },
    {
      "id": "determinants",
      "title": "Determinants: Measuring Matrix 'Size' and Orientation",
      "body": "<h1>Determinants: The Scalar Signature of a Matrix</h1><h2>Definition and Notation</h2><p>The determinant of a square matrix A, denoted det(A) or |A|, is a scalar that encodes important information about the matrix.</p><h2>Computing Determinants</h2><h3>2√ó2 Case:</h3><p>det([a b; c d]) = ad - bc</p><h3>3√ó3 Case (Cofactor Expansion):</h3><p>det(A) = a‚ÇÅ‚ÇÅC‚ÇÅ‚ÇÅ + a‚ÇÅ‚ÇÇC‚ÇÅ‚ÇÇ + a‚ÇÅ‚ÇÉC‚ÇÅ‚ÇÉ</p><p>where C·µ¢‚±º = (-1)‚Å±‚Å∫ ≤ M·µ¢‚±º and M·µ¢‚±º is the (i,j)-minor</p><h3>General: Laplace Expansion</h3><p>Expand along any row or column using cofactors.</p><h2>Properties of Determinants</h2><ul><li><strong>Row Operations:</strong> Swapping rows changes sign, scaling a row scales det</li><li><strong>Triangular Matrices:</strong> det = product of diagonal entries</li><li><strong>Product Rule:</strong> det(AB) = det(A)det(B)</li><li><strong>Transpose:</strong> det(A·µÄ) = det(A)</li></ul><h2>Geometric Interpretation</h2><ul><li><strong>2√ó2:</strong> Signed area of parallelogram formed by column vectors</li><li><strong>3√ó3:</strong> Signed volume of parallelepiped formed by column vectors</li><li><strong>General:</strong> n-dimensional signed volume</li></ul><h2>üéØ Determinant Detective</h2><p>Calculate determinants using various methods and understand their geometric meaning.</p>"
    },
    {
      "id": "inverse-determinant-connection",
      "title": "The Connection Between Inverses and Determinants",
      "body": "<h1>How Inverses and Determinants Work Together</h1><h2>The Fundamental Connection</h2><p>A square matrix A is invertible if and only if det(A) ‚â† 0.</p><h2>Cramer's Rule</h2><p>For the system Ax = b where A is invertible:</p><p>x·µ¢ = det(A·µ¢)/det(A)</p><p>where A·µ¢ is A with column i replaced by b.</p><h2>Adjugate Matrix Formula</h2><p>For invertible matrix A:</p><p>A‚Åª¬π = (1/det(A)) adj(A)</p><p>where adj(A) is the adjugate (transpose of cofactor matrix).</p><h2>Applications</h2><ul><li><strong>Solving Linear Systems:</strong> When det(A) ‚â† 0, unique solution exists</li><li><strong>Change of Variables:</strong> Determinant gives scaling factor</li><li><strong>Eigenvalue Problems:</strong> det(A - ŒªI) = 0 for eigenvalues</li></ul><h2>üéÆ Connection Explorer</h2><p>Explore how determinant values affect matrix invertibility and solution existence.</p>"
    },
    {
      "id": "vector-spaces",
      "title": "Vector Spaces: The Abstract Framework",
      "body": "<h1>Vector Spaces: Abstracting the Essential Properties</h1><h2>Definition</h2><p>A vector space V over a field F is a set with two operations (addition and scalar multiplication) satisfying eight axioms:</p><h2>Vector Space Axioms</h2><ol><li><strong>Closure under addition:</strong> u + v ‚àà V</li><li><strong>Commutativity:</strong> u + v = v + u</li><li><strong>Associativity:</strong> (u + v) + w = u + (v + w)</li><li><strong>Zero vector:</strong> ‚àÉ 0 such that v + 0 = v</li><li><strong>Additive inverse:</strong> ‚àÉ -v such that v + (-v) = 0</li><li><strong>Closure under scalar mult:</strong> cv ‚àà V</li><li><strong>Distributivity:</strong> c(u + v) = cu + cv and (c + d)v = cv + dv</li><li><strong>Scalar associativity:</strong> c(dv) = (cd)v and 1v = v</li></ol><h2>Examples of Vector Spaces</h2><ul><li><strong>‚Ñù‚Åø:</strong> n-tuples of real numbers</li><li><strong>Matrices:</strong> m√ón matrices with matrix addition</li><li><strong>Polynomials:</strong> P‚Çô = polynomials of degree ‚â§ n</li><li><strong>Functions:</strong> Real-valued functions on an interval</li></ul><h2>üéØ Vector Space Verification</h2><p>Verify that given sets with operations satisfy the vector space axioms.</p>"
    },
    {
      "id": "subspaces-formal",
      "title": "Subspaces: Formal Definition and Properties",
      "body": "<h1>Subspaces: Vector Spaces Within Vector Spaces</h1><h2>Formal Definition</h2><p>A subset H of vector space V is a subspace if H is itself a vector space under the same operations as V.</p><h2>Subspace Test (Simplified)</h2><p>H ‚äÜ V is a subspace if and only if:</p><ol><li><strong>0 ‚àà H</strong> (contains zero vector)</li><li><strong>Closed under addition:</strong> u, v ‚àà H ‚üπ u + v ‚àà H</li><li><strong>Closed under scalar multiplication:</strong> v ‚àà H, c ‚àà ‚Ñù ‚üπ cv ‚àà H</li></ol><h2>Important Subspaces</h2><ul><li><strong>Span:</strong> Span{v‚ÇÅ, ..., v‚Çñ} is always a subspace</li><li><strong>Null space:</strong> Null(A) = {x : Ax = 0}</li><li><strong>Column space:</strong> Col(A) = span of columns of A</li><li><strong>Row space:</strong> Row(A) = span of rows of A</li></ul><h2>Operations on Subspaces</h2><ul><li><strong>Intersection:</strong> H‚ÇÅ ‚à© H‚ÇÇ is a subspace</li><li><strong>Sum:</strong> H‚ÇÅ + H‚ÇÇ = {h‚ÇÅ + h‚ÇÇ : h‚ÇÅ ‚àà H‚ÇÅ, h‚ÇÇ ‚àà H‚ÇÇ}</li></ul><h2>üéÆ Subspace Detective</h2><p>Determine which subsets are actually subspaces and find their dimensions.</p>"
    },
    {
      "id": "null-column-spaces",
      "title": "Null Space and Column Space of a Matrix",
      "body": "<h1>Key Matrix Subspaces: Null and Column Spaces</h1><h2>Null Space (Kernel)</h2><p><strong>Definition:</strong> Null(A) = {x ‚àà ‚Ñù‚Åø : Ax = 0}</p><p>The null space consists of all vectors that A maps to the zero vector.</p><h3>Finding the Null Space:</h3><ol><li>Solve the homogeneous system Ax = 0</li><li>Express solution in parametric form</li><li>The parameter vectors form a basis for Null(A)</li></ol><h2>Column Space</h2><p><strong>Definition:</strong> Col(A) = span{columns of A}</p><p>The column space consists of all possible outputs of the transformation x ‚Ü¶ Ax.</p><h3>Finding the Column Space:</h3><ol><li>Find RREF of A</li><li>Identify pivot columns in RREF</li><li>Corresponding columns in original A form a basis for Col(A)</li></ol><h2>The Rank-Nullity Theorem</h2><p>For m√ón matrix A:</p><p><strong>rank(A) + nullity(A) = n</strong></p><p>where rank(A) = dim(Col(A)) and nullity(A) = dim(Null(A))</p><h2>üéØ Space Explorer</h2><p>Find bases for null and column spaces of given matrices and verify the rank-nullity theorem.</p>"
    },
    {
      "id": "linear-transformations",
      "title": "Linear Transformations: Functions That Preserve Structure",
      "body": "<h1>Linear Transformations: Structure-Preserving Functions</h1><h2>Definition</h2><p>A function T: V ‚Üí W between vector spaces is <strong>linear</strong> if:</p><ol><li><strong>Additivity:</strong> T(u + v) = T(u) + T(v)</li><li><strong>Homogeneity:</strong> T(cv) = cT(v)</li></ol><p>Equivalently: T(cu + dv) = cT(u) + dT(v)</p><h2>Matrix Representation</h2><p>Every linear transformation T: ‚Ñù‚Åø ‚Üí ‚Ñù·µê can be represented as T(x) = Ax for some m√ón matrix A.</p><p>The columns of A are T(e‚ÇÅ), T(e‚ÇÇ), ..., T(e‚Çô) where {e‚ÇÅ, ..., e‚Çô} is the standard basis.</p><h2>Examples of Linear Transformations</h2><ul><li><strong>Rotation:</strong> Rotating vectors in the plane</li><li><strong>Reflection:</strong> Reflecting across a line or plane</li><li><strong>Scaling:</strong> Stretching or shrinking by a factor</li><li><strong>Projection:</strong> Projecting onto a line or plane</li><li><strong>Shear:</strong> Skewing the coordinate system</li></ul><h2>Properties</h2><ul><li>T(0) = 0 (linear transformations map zero to zero)</li><li>T preserves linear combinations</li><li>T is completely determined by where it sends basis vectors</li></ul><h2>üéÆ Transformation Visualizer</h2><p>Explore how different matrices transform the unit square and see the geometric effects.</p>"
    },
    {
      "id": "transformation-matrices",
      "title": "Matrix Representations of Linear Transformations",
      "body": "<h1>From Transformations to Matrices</h1><h2>Standard Matrix of a Linear Transformation</h2><p>For T: ‚Ñù‚Åø ‚Üí ‚Ñù·µê, the standard matrix A is constructed by:</p><p>A = [T(e‚ÇÅ) T(e‚ÇÇ) ... T(e‚Çô)]</p><p>where e‚ÇÅ, e‚ÇÇ, ..., e‚Çô are the standard basis vectors.</p><h2>Common Transformation Matrices</h2><h3>2D Rotations (angle Œ∏):</h3><p>[cos Œ∏  -sin Œ∏]</p><p>[sin Œ∏   cos Œ∏]</p><h3>2D Reflections:</h3><ul><li><strong>Across x-axis:</strong> [1 0; 0 -1]</li><li><strong>Across y-axis:</strong> [-1 0; 0 1]</li><li><strong>Across y = x:</strong> [0 1; 1 0]</li></ul><h3>Scaling:</h3><p>[a 0; 0 b] scales by factor a in x-direction, b in y-direction</p><h2>Composition of Transformations</h2><p>If S: ‚Ñù‚Åø ‚Üí ‚Ñù·µñ and T: ‚Ñù·µñ ‚Üí ‚Ñù·µê, then:</p><p>(T ‚àò S)(x) = T(S(x))</p><p>Matrix representation: [T ‚àò S] = [T][S]</p><h2>üéØ Matrix Builder</h2><p>Given a geometric transformation description, construct its matrix representation.</p>"
    },
    {
      "id": "kernel-range",
      "title": "Kernel and Range of Linear Transformations",
      "body": "<h1>Kernel and Range: Understanding What Transformations Do</h1><h2>Kernel (Null Space)</h2><p><strong>Definition:</strong> ker(T) = {x ‚àà V : T(x) = 0}</p><p>The kernel consists of all vectors that T maps to the zero vector.</p><h3>Properties:</h3><ul><li>ker(T) is always a subspace of the domain</li><li>T is one-to-one ‚ü∫ ker(T) = {0}</li><li>For matrix transformation T(x) = Ax, ker(T) = Null(A)</li></ul><h2>Range (Image)</h2><p><strong>Definition:</strong> range(T) = {T(x) : x ‚àà V} = {y ‚àà W : T(x) = y for some x}</p><p>The range consists of all possible outputs of T.</p><h3>Properties:</h3><ul><li>range(T) is always a subspace of the codomain</li><li>T is onto ‚ü∫ range(T) = W</li><li>For matrix transformation T(x) = Ax, range(T) = Col(A)</li></ul><h2>Rank-Nullity Theorem</h2><p>For linear transformation T: V ‚Üí W where dim(V) = n:</p><p><strong>dim(ker(T)) + dim(range(T)) = n</strong></p><h2>üéÆ Kernel-Range Explorer</h2><p>Visualize kernels and ranges of various linear transformations and understand their geometric meaning.</p>"
    },
    {
      "id": "rank-nullity-theorem",
      "title": "The Rank-Nullity Theorem: A Fundamental Relationship",
      "body": "<h1>Rank-Nullity Theorem: The Great Balance</h1><h2>Statement of the Theorem</h2><p>For any linear transformation T: V ‚Üí W where V is finite-dimensional:</p><div class='theorem-box'><p><strong>dim(V) = dim(ker(T)) + dim(range(T))</strong></p></div><p>In matrix terms: <strong>n = nullity(A) + rank(A)</strong></p><h2>Intuitive Understanding</h2><p>The theorem says that the total 'dimension budget' of the domain is split between:</p><ul><li><strong>Kernel dimension:</strong> How much gets 'lost' (mapped to zero)</li><li><strong>Range dimension:</strong> How much 'survives' as output</li></ul><h2>Applications</h2><ol><li><strong>Existence of Solutions:</strong> Ax = b has solution ‚ü∫ b ‚àà Col(A)</li><li><strong>Uniqueness:</strong> If solution exists, it's unique ‚ü∫ Null(A) = {0}</li><li><strong>Invertibility:</strong> A is invertible ‚ü∫ rank(A) = n</li></ol><h2>Examples</h2><div class='example-box'><p><strong>3√ó4 matrix with rank 2:</strong></p><p>nullity = 4 - 2 = 2</p><p>So the null space is 2-dimensional (a plane through origin in ‚Ñù‚Å¥)</p></div><h2>üéØ Dimension Detective</h2><p>Given partial information about a linear transformation, use the rank-nullity theorem to find missing dimensions.</p>"
    },
    {
      "id": "eigenvalues-definition",
      "title": "Eigenvalues and Eigenvectors: Special Directions",
      "body": "<h1>Eigenvalues and Eigenvectors: The Heart of Linear Algebra</h1><h2>Definition</h2><p>For square matrix A, a nonzero vector <strong>v</strong> is an <strong>eigenvector</strong> with <strong>eigenvalue</strong> Œª if:</p><div class='eigen-equation'><p><strong>Av = Œªv</strong></p></div><p>The transformation A simply scales the eigenvector <strong>v</strong> by the factor Œª.</p><h2>Geometric Interpretation</h2><ul><li><strong>Eigenvectors:</strong> Special directions that don't change under the transformation</li><li><strong>Eigenvalues:</strong> How much the eigenvectors get scaled</li><li><strong>Œª > 1:</strong> Vector gets stretched</li><li><strong>0 < Œª < 1:</strong> Vector gets shrunk</li><li><strong>Œª < 0:</strong> Vector gets reversed and scaled</li></ul><h2>The Characteristic Equation</h2><p>To find eigenvalues, solve:</p><p><strong>det(A - ŒªI) = 0</strong></p><p>This polynomial in Œª is called the characteristic polynomial.</p><h2>Properties</h2><ul><li>An n√ón matrix has at most n distinct eigenvalues</li><li>Eigenvectors corresponding to different eigenvalues are linearly independent</li><li>The sum of eigenvalues equals the trace of A</li><li>The product of eigenvalues equals the determinant of A</li></ul><h2>üéÆ Eigen-Explorer</h2><p>Visualize how eigenvectors behave under matrix transformations and find eigenvalues graphically.</p>"
    },
    {
      "id": "finding-eigenvalues",
      "title": "Finding Eigenvalues and Eigenvectors",
      "body": "<h1>The Eigenvalue-Eigenvector Algorithm</h1><h2>Step-by-Step Process</h2><ol><li><strong>Find eigenvalues:</strong> Solve det(A - ŒªI) = 0</li><li><strong>For each eigenvalue Œª·µ¢:</strong> Solve (A - Œª·µ¢I)v = 0</li><li><strong>Find eigenspace:</strong> Null space of (A - Œª·µ¢I)</li><li><strong>Choose basis:</strong> Pick basis vectors for each eigenspace</li></ol><h2>Example Walkthrough</h2><div class='example-computation'><p><strong>Matrix:</strong> A = [3 1; 0 2]</p><p><strong>Step 1:</strong> det(A - ŒªI) = det([3-Œª 1; 0 2-Œª]) = (3-Œª)(2-Œª) = 0</p><p><strong>Eigenvalues:</strong> Œª‚ÇÅ = 3, Œª‚ÇÇ = 2</p><p><strong>Step 2:</strong> For Œª‚ÇÅ = 3: (A - 3I)v = 0 gives eigenvector [1; 0]</p><p><strong>Step 3:</strong> For Œª‚ÇÇ = 2: (A - 2I)v = 0 gives eigenvector [1; -1]</p></div><h2>Special Cases</h2><ul><li><strong>Repeated eigenvalues:</strong> May have geometric multiplicity < algebraic multiplicity</li><li><strong>Complex eigenvalues:</strong> Occur in conjugate pairs for real matrices</li><li><strong>Zero eigenvalue:</strong> Matrix is not invertible</li></ul><h2>üéØ Eigenvalue Calculator</h2><p>Practice finding eigenvalues and eigenvectors for various matrices, including challenging cases.</p>"
    },
    {
      "id": "diagonalization",
      "title": "Diagonalization: Simplifying Matrix Powers",
      "body": "<h1>Diagonalization: The Ultimate Simplification</h1><h2>What is Diagonalization?</h2><p>A matrix A is <strong>diagonalizable</strong> if there exists an invertible matrix P such that:</p><p><strong>P‚Åª¬πAP = D</strong></p><p>where D is a diagonal matrix.</p><h2>How to Diagonalize</h2><ol><li><strong>Find eigenvalues:</strong> Solve det(A - ŒªI) = 0</li><li><strong>Find eigenvectors:</strong> For each eigenvalue, find basis for eigenspace</li><li><strong>Check diagonalizability:</strong> Need n linearly independent eigenvectors</li><li><strong>Form matrices:</strong> P = [v‚ÇÅ v‚ÇÇ ... v‚Çô], D = diag(Œª‚ÇÅ, Œª‚ÇÇ, ..., Œª‚Çô)</li></ol><h2>When is Diagonalization Possible?</h2><ul><li><strong>Always possible:</strong> If A has n distinct eigenvalues</li><li><strong>Sometimes possible:</strong> If repeated eigenvalues have full geometric multiplicity</li><li><strong>Never possible:</strong> If geometric multiplicity < algebraic multiplicity for some eigenvalue</li></ul><h2>Applications</h2><ul><li><strong>Matrix powers:</strong> A‚Åø = PD‚ÅøP‚Åª¬π</li><li><strong>Systems of differential equations</strong></li><li><strong>Principal Component Analysis (PCA)</strong></li><li><strong>Google's PageRank algorithm</strong></li></ul><h2>üéÆ Diagonalization Workshop</h2><p>Practice diagonalizing matrices and compute large matrix powers efficiently.</p>"
    },
    {
      "id": "orthogonality",
      "title": "Orthogonality: When Vectors Meet at Right Angles",
      "body": "<h1>Orthogonality: The Foundation of Perpendicularity</h1><h2>Orthogonal Vectors</h2><p>Vectors <strong>u</strong> and <strong>v</strong> are <strong>orthogonal</strong> if:</p><p><strong>u ¬∑ v = 0</strong></p><h2>Orthogonal and Orthonormal Sets</h2><ul><li><strong>Orthogonal set:</strong> Vectors are mutually orthogonal</li><li><strong>Orthonormal set:</strong> Orthogonal + each vector has unit length</li></ul><h2>Gram-Schmidt Process</h2><p>Algorithm to convert any linearly independent set into an orthonormal set:</p><ol><li><strong>Normalize first vector:</strong> u‚ÇÅ = v‚ÇÅ/||v‚ÇÅ||</li><li><strong>Project and subtract:</strong> For each subsequent vector, subtract its projections onto previous orthonormal vectors</li><li><strong>Normalize result</strong></li></ol><h2>Properties of Orthogonal Sets</h2><ul><li><strong>Automatic linear independence:</strong> Orthogonal vectors (except zero) are linearly independent</li><li><strong>Easy coordinates:</strong> In orthonormal basis, coordinates are just dot products</li><li><strong>Pythagorean theorem:</strong> ||u + v||¬≤ = ||u||¬≤ + ||v||¬≤ when u ‚ä• v</li></ul><h2>üéØ Orthogonality Builder</h2><p>Use the Gram-Schmidt process to build orthonormal bases from given vectors.</p>"
    },
    {
      "id": "projections",
      "title": "Projections: Finding the Closest Point",
      "body": "<h1>Projections: Getting as Close as Possible</h1><h2>Vector Projection</h2><p>The projection of vector <strong>y</strong> onto vector <strong>u</strong> is:</p><div class='projection-formula'><p><strong>proj_u(y) = ((y ¬∑ u)/(u ¬∑ u)) u</strong></p></div><p>This gives the component of <strong>y</strong> in the direction of <strong>u</strong>.</p><h2>Projection onto Subspaces</h2><p>To project vector <strong>y</strong> onto subspace W with orthonormal basis {u‚ÇÅ, u‚ÇÇ, ..., u‚Çñ}:</p><p><strong>proj_W(y) = (y ¬∑ u‚ÇÅ)u‚ÇÅ + (y ¬∑ u‚ÇÇ)u‚ÇÇ + ... + (y ¬∑ u‚Çñ)u‚Çñ</strong></p><h2>Projection Matrix</h2><p>For subspace W = Col(A), the projection matrix is:</p><p><strong>P = A(A·µÄA)‚Åª¬πA·µÄ</strong></p><p>Then proj_W(y) = Py</p><h2>Properties</h2><ul><li><strong>Closest point:</strong> proj_W(y) is the closest point in W to y</li><li><strong>Orthogonal component:</strong> y - proj_W(y) ‚ä• W</li><li><strong>Idempotent:</strong> P¬≤ = P (projecting twice = projecting once)</li></ul><h2>Applications</h2><ul><li><strong>Least squares:</strong> Finding best-fit lines</li><li><strong>Computer graphics:</strong> Shadow calculations</li><li><strong>Signal processing:</strong> Noise reduction</li></ul><h2>üéÆ Projection Visualizer</h2><p>Visualize projections onto lines and planes, and see how they minimize distances.</p>"
    },
    {
      "id": "orthogonal-complement",
      "title": "Orthogonal Complements: The Perpendicular Space",
      "body": "<h1>Orthogonal Complements: What's Left Perpendicular</h1><h2>Definition</h2><p>For subspace W of vector space V, the <strong>orthogonal complement</strong> W‚ä• is:</p><p><strong>W‚ä• = {v ‚àà V : v ¬∑ w = 0 for all w ‚àà W}</strong></p><h2>Key Properties</h2><ul><li><strong>W‚ä• is a subspace</strong></li><li><strong>W ‚à© W‚ä• = {0}</strong></li><li><strong>If V = ‚Ñù‚Åø, then dim(W) + dim(W‚ä•) = n</strong></li><li><strong>(W‚ä•)‚ä• = W</strong> (for finite dimensions)</li></ul><h2>Finding Orthogonal Complements</h2><p>For W = span{v‚ÇÅ, v‚ÇÇ, ..., v‚Çñ} in ‚Ñù‚Åø:</p><ol><li>Form matrix A with rows v‚ÇÅ·µÄ, v‚ÇÇ·µÄ, ..., v‚Çñ·µÄ</li><li>W‚ä• = Null(A)</li><li>Solve Ax = 0 to find basis for W‚ä•</li></ol><h2>Fundamental Subspaces</h2><p>For m√ón matrix A:</p><ul><li><strong>Row(A)‚ä• = Null(A)</strong></li><li><strong>Col(A)‚ä• = Null(A·µÄ)</strong></li><li><strong>‚Ñù‚Åø = Row(A) ‚äï Null(A)</strong></li><li><strong>‚Ñù·µê = Col(A) ‚äï Null(A·µÄ)</strong></li></ul><h2>üéØ Complement Constructor</h2><p>Given a subspace, find its orthogonal complement and verify the fundamental relationships.</p>"
    },
    {
      "id": "data-science-connections",
      "title": "Linear Algebra in Data Science and Machine Learning",
      "body": "<h1>Linear Algebra: The Language of Data Science</h1><h2>ü§ñ Machine Learning Applications</h2><h3>Principal Component Analysis (PCA)</h3><ul><li><strong>Eigenvalue decomposition</strong> to find principal directions</li><li><strong>Dimensionality reduction</strong> while preserving variance</li><li><strong>Data visualization</strong> in lower dimensions</li></ul><h3>Linear Regression</h3><ul><li><strong>Normal equations:</strong> (X·µÄX)‚Åª¬πX·µÄy</li><li><strong>Least squares</strong> as orthogonal projection</li><li><strong>Regularization</strong> using matrix norms</li></ul><h3>Neural Networks</h3><ul><li><strong>Forward propagation:</strong> Matrix multiplications</li><li><strong>Backpropagation:</strong> Chain rule with matrices</li><li><strong>Weight matrices</strong> as linear transformations</li></ul><h2>üìä Data Analysis Techniques</h2><ul><li><strong>Singular Value Decomposition (SVD):</strong> Matrix factorization for recommendation systems</li><li><strong>Correlation matrices:</strong> Understanding variable relationships</li><li><strong>Covariance analysis:</strong> Portfolio optimization</li></ul><h2>üîç Real Project Examples</h2><div class='project-examples'><div class='project'><h3>Image Compression</h3><p>Use SVD to compress images by keeping only the largest singular values</p></div><div class='project'><h3>Recommendation Engine</h3><p>Matrix factorization to predict user preferences</p></div><div class='project'><h3>Face Recognition</h3><p>Eigenfaces using PCA for facial feature extraction</p></div></div><h2>üéÆ Interactive Lab</h2><p>Work with real datasets to see linear algebra concepts in action.</p>"
    },
    {
      "id": "engineering-applications",
      "title": "Linear Algebra in Engineering and Physics",
      "body": "<h1>Engineering Applications of Linear Algebra</h1><h2>üèóÔ∏è Structural Engineering</h2><h3>Finite Element Analysis</h3><ul><li><strong>Stiffness matrices:</strong> Relating forces to displacements</li><li><strong>System of equations:</strong> Kd = F (stiffness √ó displacement = force)</li><li><strong>Boundary conditions:</strong> Modifying matrices for constraints</li></ul><h3>Truss Analysis</h3><ul><li><strong>Equilibrium equations:</strong> Sum of forces = 0</li><li><strong>Matrix formulation:</strong> Large sparse systems</li><li><strong>Solution methods:</strong> Gaussian elimination for force distribution</li></ul><h2>‚ö° Electrical Engineering</h2><h3>Circuit Analysis</h3><ul><li><strong>Kirchhoff's laws</strong> as linear systems</li><li><strong>Node voltage method</strong> using matrix equations</li><li><strong>AC analysis</strong> with complex matrices</li></ul><h3>Signal Processing</h3><ul><li><strong>Fourier transforms</strong> as matrix operations</li><li><strong>Filter design</strong> using eigenvalue analysis</li><li><strong>Digital signal processing</strong> algorithms</li></ul><h2>üöÄ Control Systems</h2><ul><li><strong>State-space representation:</strong> ·∫ã = Ax + Bu</li><li><strong>Stability analysis</strong> using eigenvalues</li><li><strong>Controller design</strong> via pole placement</li></ul><h2>üéØ Engineering Challenge</h2><p>Design a simple truss structure and solve for member forces using matrix methods.</p>"
    },
    {
      "id": "svd-introduction",
      "title": "Introduction to Singular Value Decomposition",
      "body": "<h1>Singular Value Decomposition: The Ultimate Matrix Factorization</h1><h2>What is SVD?</h2><p>Every m√ón matrix A can be factored as:</p><div class='svd-formula'><p><strong>A = UŒ£V·µÄ</strong></p></div><p>where:</p><ul><li><strong>U:</strong> m√óm orthogonal matrix (left singular vectors)</li><li><strong>Œ£:</strong> m√ón diagonal matrix (singular values)</li><li><strong>V:</strong> n√ón orthogonal matrix (right singular vectors)</li></ul><h2>Geometric Interpretation</h2><p>SVD reveals that any linear transformation can be decomposed into:</p><ol><li><strong>Rotation/reflection</strong> (V ·µÄ)</li><li><strong>Scaling along coordinate axes</strong> (Œ£)</li><li><strong>Another rotation/reflection</strong> (U)</li></ol><h2>Key Properties</h2><ul><li><strong>Always exists</strong> for any matrix (unlike eigenvalue decomposition)</li><li><strong>Singular values are non-negative</strong> and arranged in decreasing order</li><li><strong>Rank equals number of non-zero singular values</strong></li></ul><h2>Applications</h2><ul><li><strong>Image compression:</strong> Keep only largest singular values</li><li><strong>Data analysis:</strong> Principal component analysis</li><li><strong>Recommendation systems:</strong> Matrix factorization</li><li><strong>Pseudoinverse:</strong> Moore-Penrose inverse for non-square matrices</li></ul><h2>üéÆ SVD Explorer</h2><p>Experiment with SVD on images and see how different numbers of singular values affect reconstruction quality.</p>"
    }
  ]
}
